\newcommand{\NWtarget}[2]{\hypertarget{#1}{#2}}
\newcommand{\NWlink}[2]{\hyperlink{#1}{#2}}
\newcommand{\NWtxtMacroDefBy}{Fragment defined by}
\newcommand{\NWtxtMacroRefIn}{Fragment referenced in}
\newcommand{\NWtxtMacroNoRef}{Fragment never referenced}
\newcommand{\NWtxtDefBy}{Defined by}
\newcommand{\NWtxtRefIn}{Referenced in}
\newcommand{\NWtxtNoRef}{Not referenced}
\newcommand{\NWtxtFileDefBy}{File defined by}
\newcommand{\NWtxtIdentsUsed}{Uses:}
\newcommand{\NWtxtIdentsNotUsed}{Never used}
\newcommand{\NWtxtIdentsDefed}{Defines:}
\newcommand{\NWsep}{${\diamond}$}
\newcommand{\NWnotglobal}{(not defined globally)}
\newcommand{\NWuseHyperlinks}{}
\documentclass[a4paper]{report}

%\VignetteIndexEntry{Stratified K-sample Inference}
%\VignetteDepends{free1way,rms,coin,multcomp,survival}
%\VignetteKeywords{conditional inference, conditional Monte Carlo}}
%\VignettePackage{free1way}

%% packages
\usepackage{amsfonts,amstext,amsmath,amssymb,amsthm}

\usepackage[utf8]{inputenc}

\newif\ifshowcode
\showcodetrue

\usepackage{latexsym}
%\usepackage{html}

\usepackage{listings}

\usepackage{color}
\definecolor{linkcolor}{rgb}{0, 0, 0.7}



\usepackage[round]{natbib}


\usepackage[%
backref,%
pageanchor=true,%
raiselinks,%
pdfhighlight=/O,%
pagebackref,%
hyperfigures,%
breaklinks,%
colorlinks,%
pdfpagemode=UseNone,%
pdfstartview=FitBH,%
linkcolor={linkcolor},%
anchorcolor={linkcolor},%
citecolor={linkcolor},%
filecolor={linkcolor},%
menucolor={linkcolor},%
urlcolor={linkcolor}%
]{hyperref}

%%% ATTENTION: no bib keys with _ allowed!
\usepackage{underscore}

\usepackage[top=25mm,bottom=25mm,left=25mm,right=25mm]{geometry}

\usepackage{lmodern}

\newcommand{\pkg}[1]{\textbf{#1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{#1}}

\newcommand{\R}{\mathbb{R} }
\newcommand{\samY}{\mathcal{Y} }
\newcommand{\Prob}{\mathbb{P} }
\newcommand{\N}{\mathbb{N} }
%\newcommand{\C}{\mathbb{C} }
\newcommand{\V}{\mathbb{V}} %% cal{\mbox{\textnormal{Var}}} }
\newcommand{\E}{\mathbb{E}} %%mathcal{\mbox{\textnormal{E}}} }
\newcommand{\Var}{\mathbb{V}} %%mathcal{\mbox{\textnormal{Var}}} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\LS}{\mathcal{L}_n}
\newcommand{\TS}{\mathcal{T}_n}
\newcommand{\LSc}{\mathcal{L}_{\text{comb},n}}
\newcommand{\LSbc}{\mathcal{L}^*_{\text{comb},n}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\yn}{y_{\text{new}}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\mH}{\mathbf{H}}
\newcommand{\mA}{\mathbf{A}}
\newcommand{\mL}{\mathbf{L}}
\newcommand{\mU}{\mathbf{U}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\xn}{\mathbf{x}_{\text{new}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\uvec}{\mathbf{u}}
\newcommand{\vvec}{\mathbf{v}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\sbullet}{\mathbin{\vcenter{\hbox{\scalebox{0.5}{$\bullet$}}}}}
\newcommand{\wdot}{\mathbf{w}_{\sbullet}}
\renewcommand{\t}{\mathbf{t}}
\newcommand{\M}{\mathbf{M}}
\renewcommand{\vec}{\text{vec}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cellx}{\pi_n[\x]}
\newcommand{\partn}{\pi_n(\mathcal{L}_n)}
\newcommand{\err}{\text{Err}}
\newcommand{\ea}{\widehat{\text{Err}}^{(a)}}
\newcommand{\ecv}{\widehat{\text{Err}}^{(cv1)}}
\newcommand{\ecvten}{\widehat{\text{Err}}^{(cv10)}}
\newcommand{\eone}{\widehat{\text{Err}}^{(1)}}
\newcommand{\eplus}{\widehat{\text{Err}}^{(.632+)}}
\newcommand{\eoob}{\widehat{\text{Err}}^{(oob)}}
\newcommand{\mub}{\boldsymbol{\mu}}
\newcommand{\Sigmab}{\boldsymbol{\Sigma}}
\def \thetavec        {\text{\boldmath$\theta$}}
\newcommand{\rT}{G}
\newcommand{\rS}{S}
\newcommand{\rt}{t}


\author{Torsten Hothorn \\ Universit\"at Z\"urich}

\title{Semiparametrically Efficient Population and Permutation Inference in 
       Distribution-free Stratified $K$-sample Oneway Layouts}

\begin{document}

\pagenumbering{roman}
\maketitle
\tableofcontents

\chapter{Model and Parameterisation}
\label{ch:model}
\pagenumbering{arabic}

We consider $K$ treatment groups $\rT \in \{1, \dots, K\}, K \ge 2$ for an
at least ordered outcome $Y \in \samY$ observed in
stratum $\rS \in \{1, \dots, B\}$ out of $B \ge 1$ blocks with conditional
cumulative distribution function (cdf)
$F_Y(y \mid \rS = b, \rT = k) = \Prob(Y \le y \mid \rS = b, \rT = k)$. Detecting
and describing differential distributions arising from different treatments
is our main objetive. We refer to the first treatment $\rT = 1$ as
``control''.

\paragraph{Model}

With model function $g: [0,1] \times \R \rightarrow [0,1]$, we describe
the conditional distribution under treatment $k$ as a function of the 
conditional distribution under control and a scalar parameter
$\delta_k$:
\begin{eqnarray*}
F(y \mid \rS = b, \rT = k) = g(F(y \mid \rS = b, \rT = 1), \delta_k).
\end{eqnarray*}
The model is assumed to hold for all blocks $b = 1,
\dots, B$, treatments $k = 2, \dots, K$, and outcome values $y \in \samY$ based on parameters
$\delta_2, \dots, \delta_K \in \R$. For notational convenience, we define $\delta_1 := 0$. 

This model formulation gives rise to several specific models, for
example, $g_\text{L}(p, \delta) = p^{\exp(-\delta)}$ (Lehmann alternatives),
$g_\text{PH}(p, \delta) = 1 - (1 -
p)^{\exp(-\delta)}$ (proportional hazards),
$g_\text{PO}(p, \delta) = \text{expit}(\text{logit}(p) - \delta)$ (proportional
odds), or $g_\text{Cd}(p, \delta) =
\Phi(\Phi^{-1}(p) - \delta)$ (generalised Cohen's $d$).

Instead of directly working with $g$, we parameterise the model in terms of
some absolute continuous cdf $F$ with log-concave density $f = F^\prime$
and corresponding derivative $f^\prime$. The location model 
\begin{eqnarray} \label{model}
F_Y(y \mid \rS = b, \rT = k) = F\left(F^{-1}\left(F_Y(y \mid \rS = b, \rT = 1)\right) - \delta_k\right), \quad k = 2, \dots, K
\end{eqnarray}
describes different distributions by means of shift parameter on a latent
scale defined by $F$. The negative shift term ensures that positive values of $\delta_k$ correspond
to the situation of outcomes being stochastically larger in group $k$
compared to control.
The shift parameters are invariant with respect to monotone transformations
of the response values, that is, transforming the observations of all
treatment groups by the same function does not affect the values of
$\delta_k$.

The choise $F(z) = \exp(-\exp(-z))$ gives rise to $g_\text{L}$, 
$F(z) = 1 - \exp(-\exp(z))$ corresponds to $g_\text{PH}$, $F = \text{expit}$
leads to  $g_\text{PO}$, and $F = \Phi$ results in $g_\text{Cd}$. The choice
of $F$ is made a priori and determines the interpretation of $\delta_k$. 

This document describes the implementation of estimators of these shift parameters,
as well as of confidence intervals and formal hypothesis tests for contrasts thereof under
the permutation and population model.

\paragraph{Hypthesis}

We are interested in inference for $\delta_2, \dots, \delta_K$, in terms of
confidence intervals and hypothesis tests of the form
\begin{eqnarray*}
& & H_0: \delta_k - \mu_k = 0, \text{``two.sided''}, \quad k = 2, \dots, K, \\
& & H_0: \delta_k - \mu_k \ge 0, \text{``less''}, \quad k = K = 2, \\
& & H_0: \delta_k - \mu_k \le 0, \text{``greater''}, \quad k = K = 2,
\end{eqnarray*}
with the latter two options only for the two-sample case ($K = 2$).

\paragraph{Likelihood}

For an ordered categorical outcome $Y$ from sample space $\samY = \{y_1 < y_2 < \cdots <
y_C\}$, we parameterise the model in terms of intercept ($\vartheta_\cdot$) and
shift ($\delta_\cdot$) parameters
\begin{eqnarray*}
F_Y(y_c \mid \rS = b, \rT = k) = F(\vartheta_{c,b} - \delta_k), \quad c = 1, \dots,
C,
\end{eqnarray*}
that is we replace the transformed control outcome $F^{-1}\left(F_Y(y_c \mid
\rS = b, \rT = 1)\right) =
\vartheta_{c,b}$ with a corresponding intercept parameter.
These $C - 1$ intercept parameters are block-specific and monotone increasing
$\vartheta_{0,b} = -\infty < \vartheta_{1,b} < \cdots < \vartheta_{C,b} = \infty$
within each block $b = 1, \dots, B$.

We collect all model parameters in a vector
\begin{eqnarray*}
\thetavec = (\theta_1 & := & \delta_2, \\
               & \dots & , \\
               \theta_{K - 1} & := & \delta_K, \\
               \theta_{K} & := & \vartheta_{1,1}, \\
               \theta_{K + 1} & := & \vartheta_{2,1} - \vartheta_{1,1} > 0, \\
               &  \dots, & \\
               \theta_{K + C - 2} & := & \vartheta_{C-1,1} - \vartheta_{C-2,1} > 0, \\
               \theta_{K + C - 1} & := & \vartheta_{1,2}, \\
               & \dots &, \\
               \theta_{B (C - 1) + K - 1} & := & \vartheta_{C-1,B} - \vartheta_{C-2,B} >
               0)
\end{eqnarray*}
featuring contrasts of the intercept parameters $\vartheta_{\cdot}$ such that monotonicity of
the intercept parameters can be ensured by box constraints for $\thetavec$.

For the $i$th observation $(y_i = y_c, s_i = b, \rt_i = k)$ from block $b$
under treatment $k$, the log-likelihood contribution is
\begin{eqnarray*}
\log(\Prob(y_{c - 1} < Y \le y_c \mid \rS = b, \rT = k)) = \log(F(\vartheta_{c,b} - \delta_k) - F(\vartheta_{c - 1,b} - \delta_k)).
\end{eqnarray*}

For an absolutely continuous outcome $Y \in \R$, we define $y_c := y_{(c)}$,
the $c$th distinct ordered observation in the sample. The log-likelihood
above is then the empirical or nonparametric log-likelihood.

If observations were independently right-censored, the contribution of the
event $Y > \tilde{y}$ to the log-likelihood is
\begin{eqnarray*}
\log(\Prob(Y > \tilde{y} \mid \rS = b, \rT = k)) = \log(1 - F(\vartheta_{c - 1,b} - \delta_k))
\end{eqnarray*}
where $y_{c - 1} = \max \{y \in \samY \mid y \le \tilde{y}\}$, that is,
observations right-censored between $y_{c - 1}$ and $y_c$ correspond to the
parameter $\vartheta_{c - 1,b}$.

Maximising this form of the log-likelihood leads to semiparametrically efficient
estimators \citep[Chapter 15.5][]{vdVaart1998}. In this framework, tests
against deviations from the hyptheses $H_0$ above are locally most
powerful rank tests, for example against proportional odds ($F =
\text{expit})$ or proportional hazards alternatives 
\citep[$F(z) = 1 - \exp(-\exp(z))$,][Example 15.16]{vdVaart1998}.

We represent the data in form of a $C \times K \times B$ contingency table,
whose element $(c, k, b)$ is the number of observations with configuration $(y = y_c, s = b,
\rt = k)$. In the presence of right-censoring, a fourth dimension is added 
($C \times K \times B \times 2)$ whose first $C \times K \times B$ table presents
right-censoring and the second table contains numbers of events.


	
\chapter{Parameter Estimation}
\label{ch:est}

<<localfun, echo = FALSE>>=
Nsim <- 100

.rcr <- function(z) {
    # Reduce('+', z, accumulate = TRUE, right = TRUE)
    # rev.default(cumsum(rev.default(z)))
    N <- length(z)
    s <- cumsum(z)
    return(s[N] - c(0, s[-N]))
}


.table2list <- function(x) {

    
    dx <- dim(x)
    if (length(dx) == 1L)
        stop("")
    if (length(dx) == 2L)
        x <- as.table(array(x, dim = c(dx, 1)))
    ms <- c(list(x), lapply(seq_along(dx), function(j) marginSums(x, j) > 0))
    ms$drop <- FALSE
    x <- do.call("[", ms)
    dx <- dim(x)
    stopifnot(length(dx) >= 3L)
    C <- dim(x)[1L]
    K <- dim(x)[2L]
    B <- dim(x)[3L]
    stopifnot(dx[1L] > 1L)
    stopifnot(K > 1L)
    xrc <- NULL
    if (length(dx) == 4L) {
        if (dx[4] == 2L) {
            xrc <- array(x[,,,"FALSE", drop = TRUE], dim = dx[1:3])
            x <- array(x[,,,"TRUE", drop = TRUE], dim = dx[1:3])
        } else {
            stop("")
        }
    }

    xlist <- xrclist <- vector(mode = "list", length = B)

    for (b in seq_len(B)) {
        xb <- matrix(x[,,b, drop = TRUE], ncol = K)
        xw <- rowSums(abs(xb)) > 0
        ### do not remove last parameter if there are corresponding
        ### right-censored observations
        if (!is.null(xrc) && any(xrc[dx[1],,b,drop = TRUE] > 0))
            xw[length(xw)] <- TRUE
        if (sum(xw) > 1L) {
            xlist[[b]] <- xb[xw,,drop = FALSE]
            attr(xlist[[b]], "idx") <- xw
            if (!is.null(xrc)) {
                xrclist[[b]] <- matrix(xrc[xw,,b,drop = TRUE], ncol = K)
                attr(xrclist[[b]], "idx") <- xw
            }
        }
    }
    strata <- !sapply(xlist, is.null)
    xlist <- xlist[strata]
    xrclist <- xrclist[strata]
    

    ret <- list(xlist = xlist)
    if (!is.null(xrc))
        ret$xrclist <- xrclist
    ret$strata <- strata
    ret
}


.nll <- function(parm, x, mu = 0, rightcensored = FALSE) {
    
    bidx <- seq_len(ncol(x) - 1L)
    delta <- c(0, mu + parm[bidx])
    intercepts <- c(-Inf, cumsum(parm[- bidx]), Inf)
    tmb <- intercepts - matrix(delta, nrow = length(intercepts),  
                                      ncol = ncol(x),
                                      byrow = TRUE)
    Ftmb <- F(tmb)
    if (rightcensored) {
        prb <- pmax(1 - Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
    } else {
        prb <- pmax(Ftmb[- 1L, , drop = FALSE] - 
                    Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
    } 
    
    return(- sum(x * log(prb)))
}


.nsc <- function(parm, x, mu = 0, rightcensored = FALSE) {
    
    bidx <- seq_len(ncol(x) - 1L)
    delta <- c(0, mu + parm[bidx])
    intercepts <- c(-Inf, cumsum(parm[- bidx]), Inf)
    tmb <- intercepts - matrix(delta, nrow = length(intercepts),  
                                      ncol = ncol(x),
                                      byrow = TRUE)
    Ftmb <- F(tmb)
    if (rightcensored) {
        prb <- pmax(1 - Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
    } else {
        prb <- pmax(Ftmb[- 1L, , drop = FALSE] - 
                    Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
    } 
    
    
    ftmb <- f(tmb)
    zu <- x * ftmb[- 1, , drop = FALSE] / prb
    if (rightcensored) zu[] <- 0 ### derivative of a constant
    zl <- x * ftmb[- nrow(ftmb), , drop = FALSE] / prb
    

    ret <- numeric(length(parm))
    ret[bidx] <- colSums(zl)[-1L] -
                 colSums(zu[-nrow(zu),,drop = FALSE])[-1L]
    ret[-bidx] <- Reduce("+", 
                         lapply(1:ncol(x), 
                             function(j) {
                                 .rcr(zu[-nrow(zu),j]) - 
                                 .rcr(zl[-1,j])
                             })
                         )
    - ret
}


.nsr <- function(parm, x, mu = 0, rightcensored = FALSE) {
    
    bidx <- seq_len(ncol(x) - 1L)
    delta <- c(0, mu + parm[bidx])
    intercepts <- c(-Inf, cumsum(parm[- bidx]), Inf)
    tmb <- intercepts - matrix(delta, nrow = length(intercepts),  
                                      ncol = ncol(x),
                                      byrow = TRUE)
    Ftmb <- F(tmb)
    if (rightcensored) {
        prb <- pmax(1 - Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
    } else {
        prb <- pmax(Ftmb[- 1L, , drop = FALSE] - 
                    Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
    } 
    
    
    ftmb <- f(tmb)
    zu <- x * ftmb[- 1, , drop = FALSE] / prb
    if (rightcensored) zu[] <- 0 ### derivative of a constant
    zl <- x * ftmb[- nrow(ftmb), , drop = FALSE] / prb
    

    ret <- rowSums(zl - zu) / rowSums(x)
    ret[!is.finite(ret)] <- 0
    - ret
}


.hes <- function(parm, x, mu = 0, rightcensored = FALSE) {
    
    bidx <- seq_len(ncol(x) - 1L)
    delta <- c(0, mu + parm[bidx])
    intercepts <- c(-Inf, cumsum(parm[- bidx]), Inf)
    tmb <- intercepts - matrix(delta, nrow = length(intercepts),  
                                      ncol = ncol(x),
                                      byrow = TRUE)
    Ftmb <- F(tmb)
    if (rightcensored) {
        prb <- pmax(1 - Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
    } else {
        prb <- pmax(Ftmb[- 1L, , drop = FALSE] - 
                    Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
    } 
    

    
    ftmb <- f(tmb)
    fptmb <- fp(tmb)

    dl <- ftmb[- nrow(ftmb), , drop = FALSE]
    du <- ftmb[- 1, , drop = FALSE]
    if (rightcensored) du[] <- 0
    dpl <- fptmb[- nrow(ftmb), , drop = FALSE]
    dpu <- fptmb[- 1, , drop = FALSE]
    if (rightcensored) dpu[] <- 0
    dlm1 <- dl[,-1L, drop = FALSE]
    dum1 <- du[,-1L, drop = FALSE]
    dplm1 <- dpl[,-1L, drop = FALSE]
    dpum1 <- dpu[,-1L, drop = FALSE]
    prbm1 <- prb[,-1L, drop = FALSE]

    i1 <- length(intercepts) - 1L
    i2 <- 1L
    

    
    Aoffdiag <- -rowSums(x * du * dl / prb^2)[-i2]
    Aoffdiag <- Aoffdiag[-length(Aoffdiag)]
    
    
    Adiag <- -rowSums((x * dpu / prb)[-i1,,drop = FALSE] - 
                      (x * dpl / prb)[-i2,,drop = FALSE] - 
                      ((x * du^2 / prb^2)[-i1,,drop = FALSE] + 
                       (x * dl^2 / prb^2)[-i2,,drop = FALSE]
                      )
                     )
                      
    
    
    xm1 <- x[,-1L,drop = FALSE] 
    X <- ((xm1 * dpum1 / prbm1)[-i1,,drop = FALSE] - 
          (xm1 * dplm1 / prbm1)[-i2,,drop = FALSE] - 
          ((xm1 * dum1^2 / prbm1^2)[-i1,,drop = FALSE] - 
           (xm1 * dum1 * dlm1 / prbm1^2)[-i2,,drop = FALSE] -
           (xm1 * dum1 * dlm1 / prbm1^2)[-i1,,drop = FALSE] +
           (xm1 * dlm1^2 / prbm1^2)[-i2,,drop = FALSE]
          )
         )

    Z <- -colSums(xm1 * (dpum1 / prbm1 - 
                         dplm1 / prbm1 -
                         (dum1^2 / prbm1^2 - 
                          2 * dum1 * dlm1 / prbm1^2 +
                          dlm1^2 / prbm1^2
                         )
                        )
                 )
    if (length(Z) > 1L) Z <- diag(Z)
    

    if (length(Adiag) > 1L) {
        if(is.null(tryCatch(loadNamespace("Matrix"), error = function(e)NULL)))
                stop(gettextf("%s needs package 'Matrix' correctly installed",
                              "free1way"),
                     domain = NA)
        A <- Matrix::bandSparse(length(Adiag), k = 0:1, diagonals = list(Adiag, Aoffdiag), 
                                symmetric = TRUE)
    } else {
        A <- matrix(Adiag)
    }
    return(list(A = A, X = X, Z = Z))
}


.snll <- function(parm, x, mu = 0, rightcensored = FALSE) {
    
    C <- sapply(x, NROW) ### might differ by stratum
    K <- unique(do.call("c", lapply(x, ncol))) ### the same
    B <- length(x)
    sidx <- factor(rep(seq_len(B), times = pmax(0, C - 1L)), levels = seq_len(B))
    bidx <- seq_len(K - 1L)
    delta <- parm[bidx]
    intercepts <- split(parm[-bidx], sidx)
    
    ret <- 0
    for (b in seq_len(B))
        ret <- ret + .nll(c(delta, intercepts[[b]]), x[[b]], mu = mu,
                          rightcensored = rightcensored)
    return(ret)
}


.snsc <- function(parm, x, mu = 0, rightcensored = FALSE) {
    
    C <- sapply(x, NROW) ### might differ by stratum
    K <- unique(do.call("c", lapply(x, ncol))) ### the same
    B <- length(x)
    sidx <- factor(rep(seq_len(B), times = pmax(0, C - 1L)), levels = seq_len(B))
    bidx <- seq_len(K - 1L)
    delta <- parm[bidx]
    intercepts <- split(parm[-bidx], sidx)
    
    ret <- numeric(length(bidx))
    for (b in seq_len(B)) {
        nsc <- .nsc(c(delta, intercepts[[b]]), x[[b]], mu = mu,
                    rightcensored = rightcensored)
        ret[bidx] <- ret[bidx] + nsc[bidx]
        ret <- c(ret, nsc[-bidx])
    }
    return(ret)
}


.shes <- function(parm, x, mu = 0, xrc = NULL) {
    
    C <- sapply(x, NROW) ### might differ by stratum
    K <- unique(do.call("c", lapply(x, ncol))) ### the same
    B <- length(x)
    sidx <- factor(rep(seq_len(B), times = pmax(0, C - 1L)), levels = seq_len(B))
    bidx <- seq_len(K - 1L)
    delta <- parm[bidx]
    intercepts <- split(parm[-bidx], sidx)
    
    ret <- matrix(0, nrow = length(bidx), ncol = length(bidx))
    for (b in seq_len(B)) {
        H <- .hes(c(delta, intercepts[[b]]), x[[b]], mu = mu)
        if (!is.null(xrc)) {
            Hrc <- .hes(c(delta, intercepts[[b]]), xrc[[b]], mu = mu, 
                        rightcensored = TRUE)
            H$X <- H$X + Hrc$X
            H$A <- H$A + Hrc$A
            H$Z <- H$Z + Hrc$Z
        }
        sAH <- try(Matrix::solve(H$A, H$X))
        if (inherits(sAH, "try-error"))
            stop("Error computing the Hessian in free1way")
        ret <- ret + (H$Z - crossprod(H$X, sAH))
    }
    as.matrix(ret)
}


.snsr <- function(parm, x, mu = 0, rightcensored = FALSE) {
    
    C <- sapply(x, NROW) ### might differ by stratum
    K <- unique(do.call("c", lapply(x, ncol))) ### the same
    B <- length(x)
    sidx <- factor(rep(seq_len(B), times = pmax(0, C - 1L)), levels = seq_len(B))
    bidx <- seq_len(K - 1L)
    delta <- parm[bidx]
    intercepts <- split(parm[-bidx], sidx)
    
    ret <- c()
    for (b in seq_len(B)) {
        idx <- attr(x[[b]], "idx")
        sr <- numeric(length(idx))
        sr[idx] <- .nsr(c(delta, intercepts[[b]]), x[[b]], mu = mu,
                        rightcensored = rightcensored)
        ret <- c(ret, sr)
    }
    return(ret)
}


.free1wayML <- function(x, link, mu = 0, start = NULL, fix = NULL, 
                        residuals = TRUE, score = TRUE, hessian = TRUE, 
                        tol = sqrt(.Machine$double.eps), ...) {

    ### convert to three-way table
    xt <- x
    stopifnot(is.table(x))
    dx <- dim(x)
    dn <- dimnames(x)
    if (length(dx) == 2L) {
        x <- as.table(array(c(x), dim = dx <- c(dx, 1L)))
        dimnames(x) <- dn <- c(dn, list(A = "A"))
    }

    ### short-cuts for link functions
    F <- function(q) .p(link, q = q)
    Q <- function(p) .q(link, p = p)
    f <- function(q) .d(link, x = q)
    fp <- function(q) .dd(link, x = q)

    

    dx <- dim(x)
    if (length(dx) == 1L)
        stop("")
    if (length(dx) == 2L)
        x <- as.table(array(x, dim = c(dx, 1)))
    ms <- c(list(x), lapply(seq_along(dx), function(j) marginSums(x, j) > 0))
    ms$drop <- FALSE
    x <- do.call("[", ms)
    dx <- dim(x)
    stopifnot(length(dx) >= 3L)
    C <- dim(x)[1L]
    K <- dim(x)[2L]
    B <- dim(x)[3L]
    stopifnot(dx[1L] > 1L)
    stopifnot(K > 1L)
    xrc <- NULL
    if (length(dx) == 4L) {
        if (dx[4] == 2L) {
            xrc <- array(x[,,,"FALSE", drop = TRUE], dim = dx[1:3])
            x <- array(x[,,,"TRUE", drop = TRUE], dim = dx[1:3])
        } else {
            stop("")
        }
    }

    xlist <- xrclist <- vector(mode = "list", length = B)

    for (b in seq_len(B)) {
        xb <- matrix(x[,,b, drop = TRUE], ncol = K)
        xw <- rowSums(abs(xb)) > 0
        ### do not remove last parameter if there are corresponding
        ### right-censored observations
        if (!is.null(xrc) && any(xrc[dx[1],,b,drop = TRUE] > 0))
            xw[length(xw)] <- TRUE
        if (sum(xw) > 1L) {
            xlist[[b]] <- xb[xw,,drop = FALSE]
            attr(xlist[[b]], "idx") <- xw
            if (!is.null(xrc)) {
                xrclist[[b]] <- matrix(xrc[xw,,b,drop = TRUE], ncol = K)
                attr(xrclist[[b]], "idx") <- xw
            }
        }
    }
    strata <- !sapply(xlist, is.null)
    xlist <- xlist[strata]
    xrclist <- xrclist[strata]
    
    if (NS <- is.null(start))
        start <- rep.int(0, K - 1)
    lwr <- rep(-Inf, times = K - 1)
    for (b in seq_len(length(xlist))) {
        bC <- nrow(xlist[[b]]) - 1L
        lwr <- c(lwr, -Inf, rep.int(0, times = bC - 1L))
        if (NS) {
            ecdf0 <- cumsum(rowSums(xlist[[b]]))
            ecdf0 <- ecdf0[-length(ecdf0)] / ecdf0[length(ecdf0)]
            Qecdf <- Q(ecdf0)
            bstart <- log(diff(Qecdf))
            start <- c(start, Qecdf[1], bstart)
            start[!is.finite(start)] <- 0
        }
    }
    
    
    .rcr <- function(z) {
        # Reduce('+', z, accumulate = TRUE, right = TRUE)
        # rev.default(cumsum(rev.default(z)))
        N <- length(z)
        s <- cumsum(z)
        return(s[N] - c(0, s[-N]))
    }
    
    
    .nll <- function(parm, x, mu = 0, rightcensored = FALSE) {
        
        bidx <- seq_len(ncol(x) - 1L)
        delta <- c(0, mu + parm[bidx])
        intercepts <- c(-Inf, cumsum(parm[- bidx]), Inf)
        tmb <- intercepts - matrix(delta, nrow = length(intercepts),  
                                          ncol = ncol(x),
                                          byrow = TRUE)
        Ftmb <- F(tmb)
        if (rightcensored) {
            prb <- pmax(1 - Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
        } else {
            prb <- pmax(Ftmb[- 1L, , drop = FALSE] - 
                        Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
        } 
        
        return(- sum(x * log(prb)))
    }
    
    
    .nsc <- function(parm, x, mu = 0, rightcensored = FALSE) {
        
        bidx <- seq_len(ncol(x) - 1L)
        delta <- c(0, mu + parm[bidx])
        intercepts <- c(-Inf, cumsum(parm[- bidx]), Inf)
        tmb <- intercepts - matrix(delta, nrow = length(intercepts),  
                                          ncol = ncol(x),
                                          byrow = TRUE)
        Ftmb <- F(tmb)
        if (rightcensored) {
            prb <- pmax(1 - Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
        } else {
            prb <- pmax(Ftmb[- 1L, , drop = FALSE] - 
                        Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
        } 
        
        
        ftmb <- f(tmb)
        zu <- x * ftmb[- 1, , drop = FALSE] / prb
        if (rightcensored) zu[] <- 0 ### derivative of a constant
        zl <- x * ftmb[- nrow(ftmb), , drop = FALSE] / prb
        

        ret <- numeric(length(parm))
        ret[bidx] <- colSums(zl)[-1L] -
                     colSums(zu[-nrow(zu),,drop = FALSE])[-1L]
        ret[-bidx] <- Reduce("+", 
                             lapply(1:ncol(x), 
                                 function(j) {
                                     .rcr(zu[-nrow(zu),j]) - 
                                     .rcr(zl[-1,j])
                                 })
                             )
        - ret
    }
    
    
    .nsr <- function(parm, x, mu = 0, rightcensored = FALSE) {
        
        bidx <- seq_len(ncol(x) - 1L)
        delta <- c(0, mu + parm[bidx])
        intercepts <- c(-Inf, cumsum(parm[- bidx]), Inf)
        tmb <- intercepts - matrix(delta, nrow = length(intercepts),  
                                          ncol = ncol(x),
                                          byrow = TRUE)
        Ftmb <- F(tmb)
        if (rightcensored) {
            prb <- pmax(1 - Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
        } else {
            prb <- pmax(Ftmb[- 1L, , drop = FALSE] - 
                        Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
        } 
        
        
        ftmb <- f(tmb)
        zu <- x * ftmb[- 1, , drop = FALSE] / prb
        if (rightcensored) zu[] <- 0 ### derivative of a constant
        zl <- x * ftmb[- nrow(ftmb), , drop = FALSE] / prb
        

        ret <- rowSums(zl - zu) / rowSums(x)
        ret[!is.finite(ret)] <- 0
        - ret
    }
    
    
    .hes <- function(parm, x, mu = 0, rightcensored = FALSE) {
        
        bidx <- seq_len(ncol(x) - 1L)
        delta <- c(0, mu + parm[bidx])
        intercepts <- c(-Inf, cumsum(parm[- bidx]), Inf)
        tmb <- intercepts - matrix(delta, nrow = length(intercepts),  
                                          ncol = ncol(x),
                                          byrow = TRUE)
        Ftmb <- F(tmb)
        if (rightcensored) {
            prb <- pmax(1 - Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
        } else {
            prb <- pmax(Ftmb[- 1L, , drop = FALSE] - 
                        Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))
        } 
        

        
        ftmb <- f(tmb)
        fptmb <- fp(tmb)

        dl <- ftmb[- nrow(ftmb), , drop = FALSE]
        du <- ftmb[- 1, , drop = FALSE]
        if (rightcensored) du[] <- 0
        dpl <- fptmb[- nrow(ftmb), , drop = FALSE]
        dpu <- fptmb[- 1, , drop = FALSE]
        if (rightcensored) dpu[] <- 0
        dlm1 <- dl[,-1L, drop = FALSE]
        dum1 <- du[,-1L, drop = FALSE]
        dplm1 <- dpl[,-1L, drop = FALSE]
        dpum1 <- dpu[,-1L, drop = FALSE]
        prbm1 <- prb[,-1L, drop = FALSE]

        i1 <- length(intercepts) - 1L
        i2 <- 1L
        

        
        Aoffdiag <- -rowSums(x * du * dl / prb^2)[-i2]
        Aoffdiag <- Aoffdiag[-length(Aoffdiag)]
        
        
        Adiag <- -rowSums((x * dpu / prb)[-i1,,drop = FALSE] - 
                          (x * dpl / prb)[-i2,,drop = FALSE] - 
                          ((x * du^2 / prb^2)[-i1,,drop = FALSE] + 
                           (x * dl^2 / prb^2)[-i2,,drop = FALSE]
                          )
                         )
                          
        
        
        xm1 <- x[,-1L,drop = FALSE] 
        X <- ((xm1 * dpum1 / prbm1)[-i1,,drop = FALSE] - 
              (xm1 * dplm1 / prbm1)[-i2,,drop = FALSE] - 
              ((xm1 * dum1^2 / prbm1^2)[-i1,,drop = FALSE] - 
               (xm1 * dum1 * dlm1 / prbm1^2)[-i2,,drop = FALSE] -
               (xm1 * dum1 * dlm1 / prbm1^2)[-i1,,drop = FALSE] +
               (xm1 * dlm1^2 / prbm1^2)[-i2,,drop = FALSE]
              )
             )

        Z <- -colSums(xm1 * (dpum1 / prbm1 - 
                             dplm1 / prbm1 -
                             (dum1^2 / prbm1^2 - 
                              2 * dum1 * dlm1 / prbm1^2 +
                              dlm1^2 / prbm1^2
                             )
                            )
                     )
        if (length(Z) > 1L) Z <- diag(Z)
        

        if (length(Adiag) > 1L) {
            if(is.null(tryCatch(loadNamespace("Matrix"), error = function(e)NULL)))
                    stop(gettextf("%s needs package 'Matrix' correctly installed",
                                  "free1way"),
                         domain = NA)
            A <- Matrix::bandSparse(length(Adiag), k = 0:1, diagonals = list(Adiag, Aoffdiag), 
                                    symmetric = TRUE)
        } else {
            A <- matrix(Adiag)
        }
        return(list(A = A, X = X, Z = Z))
    }
    
    
    .snll <- function(parm, x, mu = 0, rightcensored = FALSE) {
        
        C <- sapply(x, NROW) ### might differ by stratum
        K <- unique(do.call("c", lapply(x, ncol))) ### the same
        B <- length(x)
        sidx <- factor(rep(seq_len(B), times = pmax(0, C - 1L)), levels = seq_len(B))
        bidx <- seq_len(K - 1L)
        delta <- parm[bidx]
        intercepts <- split(parm[-bidx], sidx)
        
        ret <- 0
        for (b in seq_len(B))
            ret <- ret + .nll(c(delta, intercepts[[b]]), x[[b]], mu = mu,
                              rightcensored = rightcensored)
        return(ret)
    }
    
    
    .snsc <- function(parm, x, mu = 0, rightcensored = FALSE) {
        
        C <- sapply(x, NROW) ### might differ by stratum
        K <- unique(do.call("c", lapply(x, ncol))) ### the same
        B <- length(x)
        sidx <- factor(rep(seq_len(B), times = pmax(0, C - 1L)), levels = seq_len(B))
        bidx <- seq_len(K - 1L)
        delta <- parm[bidx]
        intercepts <- split(parm[-bidx], sidx)
        
        ret <- numeric(length(bidx))
        for (b in seq_len(B)) {
            nsc <- .nsc(c(delta, intercepts[[b]]), x[[b]], mu = mu,
                        rightcensored = rightcensored)
            ret[bidx] <- ret[bidx] + nsc[bidx]
            ret <- c(ret, nsc[-bidx])
        }
        return(ret)
    }
    
    
    .shes <- function(parm, x, mu = 0, xrc = NULL) {
        
        C <- sapply(x, NROW) ### might differ by stratum
        K <- unique(do.call("c", lapply(x, ncol))) ### the same
        B <- length(x)
        sidx <- factor(rep(seq_len(B), times = pmax(0, C - 1L)), levels = seq_len(B))
        bidx <- seq_len(K - 1L)
        delta <- parm[bidx]
        intercepts <- split(parm[-bidx], sidx)
        
        ret <- matrix(0, nrow = length(bidx), ncol = length(bidx))
        for (b in seq_len(B)) {
            H <- .hes(c(delta, intercepts[[b]]), x[[b]], mu = mu)
            if (!is.null(xrc)) {
                Hrc <- .hes(c(delta, intercepts[[b]]), xrc[[b]], mu = mu, 
                            rightcensored = TRUE)
                H$X <- H$X + Hrc$X
                H$A <- H$A + Hrc$A
                H$Z <- H$Z + Hrc$Z
            }
            sAH <- try(Matrix::solve(H$A, H$X))
            if (inherits(sAH, "try-error"))
                stop("Error computing the Hessian in free1way")
            ret <- ret + (H$Z - crossprod(H$X, sAH))
        }
        as.matrix(ret)
    }
    
    
    .snsr <- function(parm, x, mu = 0, rightcensored = FALSE) {
        
        C <- sapply(x, NROW) ### might differ by stratum
        K <- unique(do.call("c", lapply(x, ncol))) ### the same
        B <- length(x)
        sidx <- factor(rep(seq_len(B), times = pmax(0, C - 1L)), levels = seq_len(B))
        bidx <- seq_len(K - 1L)
        delta <- parm[bidx]
        intercepts <- split(parm[-bidx], sidx)
        
        ret <- c()
        for (b in seq_len(B)) {
            idx <- attr(x[[b]], "idx")
            sr <- numeric(length(idx))
            sr[idx] <- .nsr(c(delta, intercepts[[b]]), x[[b]], mu = mu,
                            rightcensored = rightcensored)
            ret <- c(ret, sr)
        }
        return(ret)
    }
    
    
    fn <- function(par) {
        par[is.finite(lwr)] <- exp(par[is.finite(lwr)])
        ret <- .snll(par, x = xlist, mu = mu)
        if (!is.null(xrc))
            ret <- ret + .snll(par, x = xrclist, mu = mu, 
                               rightcensored = TRUE)
        return(ret)
    }
    gr <- function(par) {
        par[is.finite(lwr)] <- exp(par[is.finite(lwr)])
        ret <- .snsc(par, x = xlist, mu = mu)
        if (!is.null(xrc))
            ret <- ret + .snsc(par, x = xrclist, mu = mu, 
                               rightcensored = TRUE)
        par[!is.finite(lwr)] <- 1
        return(ret * par)
    }
    .profile <- function(start, fix = seq_len(K - 1)) {
        stopifnot(all(fix %in% seq_len(K - 1)))
        delta <- start[fix]
        opargs <- c(list(par = start[-fix], 
                         fn = function(par) {
                             p <- numeric(length(par) + length(fix))
                             p[fix] <- delta
                             p[-fix] <- par
                             fn(p)
                         },
                         gr = function(par) {
                             p <- numeric(length(par) + length(fix))
                             p[fix] <- delta
                             p[-fix] <- par
                             gr(p)[-fix]
                         },
                         method = "BFGS", 
                         hessian = FALSE),
                         list(...))
        
        maxit <- 100
        while(maxit < 10001) {
           ret <- do.call("optim", opargs)
           maxit <- 5 * maxit
           if (ret$convergence > 0) {
               if (is.null(opargs$control))
                   opargs$control <- list(maxit = maxit)
                else 
                   opargs$control$maxit <- maxit
               opargs$par <- ret$par
           } else {
               break()
           }
        }
        if (ret$convergence > 0)
            stop(paste("Unsuccessful optimisation in free1way", ret$message))
        
        p <- numeric(length(start))
        p[fix] <- delta
        p[-fix] <- ret$par
        ret$par <- p
        ret
    }
    
    
    if (!length(fix)) {
        opargs <- c(list(par = start, fn = fn, gr = gr,
                         # lower = lwr, 
                         method = "BFGS", 
                         hessian = FALSE), list(...))
        
        maxit <- 100
        while(maxit < 10001) {
           ret <- do.call("optim", opargs)
           maxit <- 5 * maxit
           if (ret$convergence > 0) {
               if (is.null(opargs$control))
                   opargs$control <- list(maxit = maxit)
                else 
                   opargs$control$maxit <- maxit
               opargs$par <- ret$par
           } else {
               break()
           }
        }
        if (ret$convergence > 0)
            stop(paste("Unsuccessful optimisation in free1way", ret$message))
        
    } else if (length(fix) == length(start)) {
        ret <- list(par = start, 
                    value = fn(start))
    } else {
        ret <- .profile(start, fix = fix)
    }
     
    
    if (is.null(fix) || (length(fix) == length(start)))
        parm <- seq_len(K - 1)
    else 
        parm <- fix
    if (any(parm >= K)) return(ret)

    ret$coefficients <- ret$par[parm]
    dn2 <- dimnames(xt)[2L]
    names(ret$coefficients) <- cnames <- paste0(names(dn2), dn2[[1L]][1L + parm])

    par <- ret$par
    par[is.finite(lwr)] <- exp(par[is.finite(lwr)])

    if (score) {
        ret$negscore <- .snsc(par, x = xlist, mu = mu)[parm]
        if (!is.null(xrc))
            ret$negscore <- ret$negscore + .snsc(par, x = xrclist, mu = mu, 
                                                 rightcensored = TRUE)[parm]
    }
    if (hessian) {
        if (!is.null(xrc)) {
            ret$hessian <- .shes(par, x = xlist, mu = mu, xrc = xrclist)
        } else {
            ret$hessian <- .shes(par, x = xlist, mu = mu)
        }
        ret$vcov <- solve(ret$hessian)
        if (length(parm) != nrow(ret$hessian))
           ret$hessian <- solve(ret$vcov <- ret$vcov[parm, parm, drop = FALSE])
        rownames(ret$vcov) <- colnames(ret$vcov) <- rownames(ret$hessian) <-
            colnames(ret$hessian) <-  cnames
    }
    if (residuals) {
        ret$negresiduals <- .snsr(par, x = xlist, mu = mu)
        if (!is.null(xrc)) {
            rcr <- .snsr(par, x = xrclist, mu = mu, rightcensored = TRUE)
            ret$negresiduals <- c(rbind(matrix(ret$negresiduals, nrow = C),
                                        matrix(rcr, nrow = C)))
         }
    }
    ret$profile <- function(start, fix)
        .free1wayML(xt, link = link, mu = mu, start = start, fix = fix, tol = tol, 
                   ...) 
    ret$table <- xt
    ret$mu <- mu
    ret$strata <- strata
    names(ret$mu) <- link$parm
    

    class(ret) <- "free1wayML"
    ret
}


.SW <- function(res, xt) {

    if (length(dim(xt)) == 3L) {
        res <- matrix(res, nrow = dim(xt)[1L], ncol = dim(xt)[3])
        STAT <-  Exp <- Cov <- 0
        for (b in seq_len(dim(xt)[3L])) {
            sw <- .SW(res[,b, drop = TRUE], xt[,,b, drop = TRUE])
            STAT <- STAT + sw$Statistic
            Exp <- Exp + sw$Expectation
            Cov <- Cov + sw$Covariance
        }
        return(list(Statistic = STAT, Expectation = as.vector(Exp),
                    Covariance = Cov))
    }

    Y <- matrix(res, ncol = 1, nrow = length(xt))
    weights <- c(xt)
    x <- gl(ncol(xt), nrow(xt))
    X <- model.matrix(~ x, data = data.frame(x = x))[,-1L,drop = FALSE]

    w. <- sum(weights)
    wX <- weights * X
    wY <- weights * Y
    ExpX <- colSums(wX)
    ExpY <- colSums(wY) / w.
    CovX <- crossprod(X, wX)
    Yc <- t(t(Y) - ExpY)
    CovY <- crossprod(Yc, weights * Yc) / w.
    Exp <- kronecker(ExpY, ExpX)
    Cov <- w. / (w. - 1) * kronecker(CovY, CovX) -
           1 / (w. - 1) * kronecker(CovY, tcrossprod(ExpX))
    STAT <- crossprod(X, wY)
    list(Statistic = STAT, Expectation = as.vector(Exp),
         Covariance = Cov)
}


.resample <- function(res, xt, B = 10000) {

    if (length(dim(xt)) == 2L)
        xt <- as.table(array(xt, dim = c(dim(xt), 1)))

    res <- matrix(res, nrow = dim(xt)[1L], ncol = dim(xt)[3L])
    stat <- 0
    ret <- .SW(res, xt)
    if (dim(xt)[2L] == 2L) {
        ret$testStat <- c((ret$Statistic - ret$Expectation) / sqrt(c(ret$Covariance)))
    } else {
        ES <- ret$Statistic - ret$Expectation
        ret$testStat <- sum(ES * solve(ret$Covariance, ES))
    }
    ret$DF <- dim(xt)[2L] - 1L

    if (B) {
        for (j in 1:dim(xt)[3L]) {
           rt <- r2dtable(B, r = rowSums(xt[,,j]), c = colSums(xt[,,j]))
           stat <- stat + sapply(rt, function(x) colSums(x[,-1L, drop = FALSE] * res[,j]))
        }
        if (dim(xt)[2L] == 2L) {
             ret$permStat <- (stat - ret$Expectation) / sqrt(c(ret$Covariance))
        } else {
            ES <- matrix(stat, ncol = B) - ret$Expectation
            ret$permStat <- rowSums(crossprod(ES, solve(ret$Covariance, ES)))
        }
    }
    ret
}


.p <- function(link, q, ...)
    link$linkinv(q = q, ...)

.q <- function(link, p, ...)
    link$link(p = p, ...)

.d <- function(link, x, ...)
    link$dlinkinv(x = x, ...)

.dd <- function(link, x, ...)
    link$ddlinkinv(x = x, ...)

.ddd <- function(link, x, ...)
    link$dddlinkinv(x = x, ...)

.dd2d <- function(link, x, ...)
    link$dd2dlinkinv(x = x, ...)

linkfun <- function(alias, 
                    model, 
                    parm, 
                    link, 
                    linkinv,
                    dlinkinv, 
                    ddlinkinv,
                    ...) {

    ret <- list(alias = alias,
                model = model,
                parm = parm,
                link = link,
                linkinv = linkinv,
                dlinkinv = dlinkinv,
                ddlinkinv = ddlinkinv)
    if (is.null(ret$dd2d)) 
        ret$dd2d <- function(x) 
            ret$ddlinkinv(x) / ret$dlinkinv(x)
    ret <- c(ret, list(...))
    class(ret) <- "linkfun"
    ret
}


logit <- function()
    linkfun(alias = c("Wilcoxon", "Kruskal-Wallis"),
            model = "proportional odds", 
            parm = "log-odds ratio",
            link = qlogis,
            linkinv = plogis,
            dlinkinv = dlogis,
            ddlinkinv = function(x) {
                p <- plogis(x)
                p * (1 - p)^2 - p^2 * (1 - p)
            },
            dddlinkinv = function(x) {
                ex <- exp(x)
                ifelse(is.finite(x), (ex - 4 * ex^2 + ex^3) / (1 + ex)^4, 0.0)
            },
            dd2d = function(x) {
                ex <- exp(x)
                (1 - ex) / (1 + ex)
            },
            parm2PI = function(x) {
               OR <- exp(x)
               ret <- OR * (OR - 1 - x)/(OR - 1)^2
               ret[abs(x) < .Machine$double.eps] <- 0.5
               return(ret)
            },
            PI2parm = function(p) {
               f <- function(x, PI)
                   x + (exp(-x) * (PI + exp(2 * x) * (PI - 1) + exp(x)* (1 - 2 * PI)))
               ret <- sapply(p, function(p) 
                   uniroot(f, PI = p, interval = 50 * c(-1, 1))$root)
               return(ret)
            },
            parm2OVL = function(x) 2 * plogis(-abs(x / 2))
    )


probit <- function()
    linkfun(alias = "van der Waerden normal scores",
            model = "latent normal shift", 
            parm = "generalised Cohen's d",
            link = qnorm,
            linkinv = pnorm,
            dlinkinv = dnorm,
            ddlinkinv = function(x) 
                ifelse(is.finite(x), -dnorm(x = x) * x, 0.0), 
            dddlinkinv = function(x) 
                ifelse(is.finite(x), dnorm(x = x) * (x^2 - 1), 0.0),
            dd2d = function(x) -x,
            parm2PI = function(x) pnorm(x, sd = sqrt(2)),
            PI2parm = function(p) qnorm(p, sd = sqrt(2)),
            parm2OVL = function(x) 2 * pnorm(-abs(x / 2))
    )


cloglog <- function()
    linkfun(alias = "Savage",
            model = "proportional hazards", 
            parm = "log-hazard ratio",
            link = function(p, log.p = FALSE) {
                if (log.p) p <- exp(p)
                log(-log1p(- p))
            },
            linkinv = function(q, lower.tail = TRUE, log.p = FALSE) {
                ### p = 1 - exp(-exp(q))
                ret <- exp(-exp(q))
                if (log.p) {
                    if (lower.tail)
                        return(log1p(-ret))
                    return(-exp(q))
                }
                if (lower.tail)
                    return(-expm1(-exp(q)))
                return(ret)
            },
            dlinkinv = function(x) 
                ifelse(is.finite(x), exp(x - exp(x)), 0.0),
            ddlinkinv = function(x) {
                ex <- exp(x)
                ifelse(is.finite(x), (ex - ex^2) / exp(ex), 0.0)
            },
            dddlinkinv = function(x) {
                ex <- exp(x)
                ifelse(is.finite(x), (ex - 3*ex^2 + ex^3) / exp(ex), 0.0)
            },
            dd2d = function(x)
               -expm1(x),
            parm2PI = plogis,
            PI2parm = qlogis,
            parm2OVL = function(x) {
                x <- abs(x)
                ret <- exp(x / (exp(-x) - 1)) - exp(-x / (exp(x) - 1)) + 1 
                ret[abs(x) < .Machine$double.eps] <- 1
                x[] <- ret
                return(x)
            }
    )


loglog <- function()
    linkfun(alias = "Lehmann", 
            model = "Lehmann", 
            parm = "log-reverse time hazard ratio",
            link = function(p, log.p = FALSE) {
                if (!log.p) p <- log(p)
                -log(-p)
            },
            linkinv = function(q, lower.tail = TRUE, log.p = FALSE) {
                ### p = exp(-exp(-q))
                if (log.p) {
                    if (lower.tail)
                        return(-exp(-q))
                    return(log1p(-exp(-exp(-q))))
                }
                if (lower.tail)
                    return(exp(-exp(-q)))
                -expm1(-exp(-q))
            },
            dlinkinv = function(x) 
                ifelse(is.finite(x), exp(- x - exp(-x)), 0.0),
            ddlinkinv = function(x) {
               ex <- exp(-x)
               ifelse(is.finite(x), exp(-ex - x) * (ex - 1.0), 0.0)
            },
            dddlinkinv = function(x) {
               ex <- exp(-x)
               ifelse(is.finite(x), exp(-x - ex) * (ex - 1)^2 - exp(-ex - 2 * x), 0.0)
            },
            dd2d = function(x) 
                expm1(-x),
            parm2PI = plogis,
            PI2parm = qlogis,
            parm2OVL = function(x) {
                x <- abs(x)
                rt <- exp(-x / (exp(x) - 1))
                ret <- rt^exp(x) + 1 - rt
                ret[abs(x) < .Machine$double.eps] <- 1
                x[] <- ret
                return(x)
            }
    )

@

We start implementing the log-likelihood function for parameters \code{parm}
$= \thetavec$ (assuming only a single block) with data from a two-way $C
\times K$ contingency table \code{x}. 

From $\thetavec$, we first extract the shift parameters $\delta_\cdot$ and
then the intercept parameters $\vartheta_\cdot$, compute the differences
$\vartheta_{c,1} - \delta_k$ and evaluate the probabilities
\code{prb} $ = \Prob(y_{c - 1} < Y \le y_c \mid \rS = 1, \rT = k)$ for all
groups:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap1}\raggedright\small
\NWtarget{nuweb3a}{} $\langle\,${\itshape parm to prob}\nobreak\ {\footnotesize {3a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@bidx <- seq_len(ncol(x) - 1L)@\\
\mbox{}\verb@delta <- c(0, mu + parm[bidx])@\\
\mbox{}\verb@intercepts <- c(-Inf, cumsum(parm[- bidx]), Inf)@\\
\mbox{}\verb@tmb <- intercepts - matrix(delta, nrow = length(intercepts),  @\\
\mbox{}\verb@                                  ncol = ncol(x),@\\
\mbox{}\verb@                                  byrow = TRUE)@\\
\mbox{}\verb@Ftmb <- F(tmb)@\\
\mbox{}\verb@if (rightcensored) {@\\
\mbox{}\verb@    prb <- pmax(1 - Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))@\\
\mbox{}\verb@} else {@\\
\mbox{}\verb@    prb <- pmax(Ftmb[- 1L, , drop = FALSE] - @\\
\mbox{}\verb@                Ftmb[- nrow(Ftmb), , drop = FALSE], sqrt(.Machine$double.eps))@\\
\mbox{}\verb@} @\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb3b}{3b}\NWlink{nuweb4b}{, 4b}\NWlink{nuweb5a}{, 5a}\NWlink{nuweb7}{, 7}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
If the table \code{x} represents right-censored observations, we compute
\code{prb} $ = 1 - \Prob(Y \le y_c \mid \rS = 1, \rT = k)$.

With default null values $\mu_k = 0, k = 2, \dots, K$, we define the
negative log-likelihood function as the weighted (by number of observations) sum of
the log-probabilities

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap2}\raggedright\small
\NWtarget{nuweb3b}{} $\langle\,${\itshape negative logLik}\nobreak\ {\footnotesize {3b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.nll <- function(parm, x, mu = 0, rightcensored = FALSE) {@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape parm to prob}\nobreak\ {\footnotesize \NWlink{nuweb3a}{3a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    return(- sum(x * log(prb)))@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The code assumes that all elements of the margins of the table \code{x} are larger than
zero; otherwise, the corresponding parameter is not identified. We will
handle such situation at a higher level later on.

It is important to note that, with $F$ corresponding to distribution with
log-concave density $f$, the negative log-likelihood is a convex function of
the parameters $\thetavec$, and thus we can solve the corresponding
constrained minimisation problem quickly and reliably.

Next, we implement the gradient of the negative
log-likelihood, the negative score function for the parameters in
$\thetavec$. The score function for the empirical likelihood, evaluated at
parameters $\vartheta_\cdot$ and $\delta_\cdot$ is given in many places
\citep[for example in][Formula~(2)]{HothornMoestBuehlmann2017}. 
We begin computing the ratio of $f(\vartheta_{c,1} -
\delta_k)$ and the corresponding likelihood

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap3}\raggedright\small
\NWtarget{nuweb4a}{} $\langle\,${\itshape density prob ratio}\nobreak\ {\footnotesize {4a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@ftmb <- f(tmb)@\\
\mbox{}\verb@zu <- x * ftmb[- 1, , drop = FALSE] / prb@\\
\mbox{}\verb@if (rightcensored) zu[] <- 0 ### derivative of a constant@\\
\mbox{}\verb@zl <- x * ftmb[- nrow(ftmb), , drop = FALSE] / prb@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb4b}{4b}\NWlink{nuweb5a}{, 5a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
and then compute the negative score function:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap4}\raggedright\small
\NWtarget{nuweb4b}{} $\langle\,${\itshape negative score}\nobreak\ {\footnotesize {4b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.nsc <- function(parm, x, mu = 0, rightcensored = FALSE) {@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape parm to prob}\nobreak\ {\footnotesize \NWlink{nuweb3a}{3a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape density prob ratio}\nobreak\ {\footnotesize \NWlink{nuweb4a}{4a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ret <- numeric(length(parm))@\\
\mbox{}\verb@    ret[bidx] <- colSums(zl)[-1L] -@\\
\mbox{}\verb@                 colSums(zu[-nrow(zu),,drop = FALSE])[-1L]@\\
\mbox{}\verb@    ret[-bidx] <- Reduce("+", @\\
\mbox{}\verb@                         lapply(1:ncol(x), @\\
\mbox{}\verb@                             function(j) {@\\
\mbox{}\verb@                                 .rcr(zu[-nrow(zu),j]) - @\\
\mbox{}\verb@                                 .rcr(zl[-1,j])@\\
\mbox{}\verb@                             })@\\
\mbox{}\verb@                         )@\\
\mbox{}\verb@    - ret@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
Adjustment for the parameterisation in terms of differences between
intercepts needs this small helper function:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap5}\raggedright\small
\NWtarget{nuweb4c}{} $\langle\,${\itshape cumsumrev}\nobreak\ {\footnotesize {4c}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.rcr <- function(z) {@\\
\mbox{}\verb@    # Reduce('+', z, accumulate = TRUE, right = TRUE)@\\
\mbox{}\verb@    # rev.default(cumsum(rev.default(z)))@\\
\mbox{}\verb@    N <- length(z)@\\
\mbox{}\verb@    s <- cumsum(z)@\\
\mbox{}\verb@    return(s[N] - c(0, s[-N]))@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
(<TH>maybe add \code{rev = TRUE} to \code{cumsum}?</TH>).

In addition, we define negative score residuals, that is, the derivative of the
negative log-likelihood with respect to an intercept term constrained to
zero:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap6}\raggedright\small
\NWtarget{nuweb5a}{} $\langle\,${\itshape negative score residuals}\nobreak\ {\footnotesize {5a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.nsr <- function(parm, x, mu = 0, rightcensored = FALSE) {@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape parm to prob}\nobreak\ {\footnotesize \NWlink{nuweb3a}{3a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape density prob ratio}\nobreak\ {\footnotesize \NWlink{nuweb4a}{4a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ret <- rowSums(zl - zu) / rowSums(x)@\\
\mbox{}\verb@    ret[!is.finite(ret)] <- 0@\\
\mbox{}\verb@    - ret@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
We also need access to the observed Fisher information of the shift
parameters. We proceed by implementing the Hessian for the intercept
($\vartheta_\cdot$) and shift ($\delta_\cdot$) parameters, as given in Formula~(4) of
\cite{HothornMoestBuehlmann2017} first. This partitioned matrix
\begin{eqnarray*}
\mH(\vartheta_1, \dots, \vartheta_{C - 1}, \delta_2, \dots, \delta_K) = 
\left(\begin{array}{ll}
\mA & \X \\
\X^\top & \Z
\end{array} \right)
\end{eqnarray*}
consists of a symmetric tridiagonal $\mA \sim (C-1,C-1)$, a diagonal $\Z \sim (K - 1, K -
1)$, and a full $\X \sim (C - 1, K - 1)$ matrix. In a second step, we
compute the Fisher information matrix for the shift parameters only by means
of the Schur complement $\Z - \X^\top \mA^{-1} \X$.

In addition to probabilities \code{prb}, the Hessian necessitates the
computation of $f(\vartheta_{c,1} - \delta_k)$ and $f^\prime(\vartheta_{c,1} -
\delta_k)$. We start preparing these objects, keeping in mind to remove terms
not being present under right-censoring:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap7}\raggedright\small
\NWtarget{nuweb5b}{} $\langle\,${\itshape Hessian prep}\nobreak\ {\footnotesize {5b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@ftmb <- f(tmb)@\\
\mbox{}\verb@fptmb <- fp(tmb)@\\
\mbox{}\verb@@\\
\mbox{}\verb@dl <- ftmb[- nrow(ftmb), , drop = FALSE]@\\
\mbox{}\verb@du <- ftmb[- 1, , drop = FALSE]@\\
\mbox{}\verb@if (rightcensored) du[] <- 0@\\
\mbox{}\verb@dpl <- fptmb[- nrow(ftmb), , drop = FALSE]@\\
\mbox{}\verb@dpu <- fptmb[- 1, , drop = FALSE]@\\
\mbox{}\verb@if (rightcensored) dpu[] <- 0@\\
\mbox{}\verb@dlm1 <- dl[,-1L, drop = FALSE]@\\
\mbox{}\verb@dum1 <- du[,-1L, drop = FALSE]@\\
\mbox{}\verb@dplm1 <- dpl[,-1L, drop = FALSE]@\\
\mbox{}\verb@dpum1 <- dpu[,-1L, drop = FALSE]@\\
\mbox{}\verb@prbm1 <- prb[,-1L, drop = FALSE]@\\
\mbox{}\verb@@\\
\mbox{}\verb@i1 <- length(intercepts) - 1L@\\
\mbox{}\verb@i2 <- 1L@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb7}{7}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The off-diagonal elements of $\mA$ are now available as
\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap8}\raggedright\small
\NWtarget{nuweb5c}{} $\langle\,${\itshape off-diagonal elements for Hessian of intercepts}\nobreak\ {\footnotesize {5c}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@Aoffdiag <- -rowSums(x * du * dl / prb^2)[-i2]@\\
\mbox{}\verb@Aoffdiag <- Aoffdiag[-length(Aoffdiag)]@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb7}{7}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
and the diagonal elements of $\mA$ as
\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap9}\raggedright\small
\NWtarget{nuweb6a}{} $\langle\,${\itshape diagonal elements for Hessian of intercepts}\nobreak\ {\footnotesize {6a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@Adiag <- -rowSums((x * dpu / prb)[-i1,,drop = FALSE] - @\\
\mbox{}\verb@                  (x * dpl / prb)[-i2,,drop = FALSE] - @\\
\mbox{}\verb@                  ((x * du^2 / prb^2)[-i1,,drop = FALSE] + @\\
\mbox{}\verb@                   (x * dl^2 / prb^2)[-i2,,drop = FALSE]@\\
\mbox{}\verb@                  )@\\
\mbox{}\verb@                 )@\\
\mbox{}\verb@                  @\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb7}{7}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
For the computation of $\X$ and $\Z$, the observations corresponding to the
control group ($k = 1$) are irrelevant, we remove these first

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap10}\raggedright\small
\NWtarget{nuweb6b}{} $\langle\,${\itshape intercept / shift contributions to Hessian}\nobreak\ {\footnotesize {6b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@xm1 <- x[,-1L,drop = FALSE] @\\
\mbox{}\verb@X <- ((xm1 * dpum1 / prbm1)[-i1,,drop = FALSE] - @\\
\mbox{}\verb@      (xm1 * dplm1 / prbm1)[-i2,,drop = FALSE] - @\\
\mbox{}\verb@      ((xm1 * dum1^2 / prbm1^2)[-i1,,drop = FALSE] - @\\
\mbox{}\verb@       (xm1 * dum1 * dlm1 / prbm1^2)[-i2,,drop = FALSE] -@\\
\mbox{}\verb@       (xm1 * dum1 * dlm1 / prbm1^2)[-i1,,drop = FALSE] +@\\
\mbox{}\verb@       (xm1 * dlm1^2 / prbm1^2)[-i2,,drop = FALSE]@\\
\mbox{}\verb@      )@\\
\mbox{}\verb@     )@\\
\mbox{}\verb@@\\
\mbox{}\verb@Z <- -colSums(xm1 * (dpum1 / prbm1 - @\\
\mbox{}\verb@                     dplm1 / prbm1 -@\\
\mbox{}\verb@                     (dum1^2 / prbm1^2 - @\\
\mbox{}\verb@                      2 * dum1 * dlm1 / prbm1^2 +@\\
\mbox{}\verb@                      dlm1^2 / prbm1^2@\\
\mbox{}\verb@                     )@\\
\mbox{}\verb@                    )@\\
\mbox{}\verb@             )@\\
\mbox{}\verb@if (length(Z) > 1L) Z <- diag(Z)@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb7}{7}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
We return the three matrices $\mA$, $\X$, and $\Z$ necessary for the
computation of the Fisher information for $\delta_2, \dots, \delta_K$ as the Schur
complement $\Z - \X^\top \mA^{-1} \X$. Because the matrix $\mA$ is symmetric
tridiagonal, we use infrastructure from the \pkg{Matrix} package to
represent this matrix:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap11}\raggedright\small
\NWtarget{nuweb7}{} $\langle\,${\itshape Hessian}\nobreak\ {\footnotesize {7}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.hes <- function(parm, x, mu = 0, rightcensored = FALSE) {@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape parm to prob}\nobreak\ {\footnotesize \NWlink{nuweb3a}{3a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape Hessian prep}\nobreak\ {\footnotesize \NWlink{nuweb5b}{5b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape off-diagonal elements for Hessian of intercepts}\nobreak\ {\footnotesize \NWlink{nuweb5c}{5c}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape diagonal elements for Hessian of intercepts}\nobreak\ {\footnotesize \NWlink{nuweb6a}{6a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape intercept / shift contributions to Hessian}\nobreak\ {\footnotesize \NWlink{nuweb6b}{6b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (length(Adiag) > 1L) {@\\
\mbox{}\verb@        if(is.null(tryCatch(loadNamespace("Matrix"), error = function(e)NULL)))@\\
\mbox{}\verb@                stop(gettextf("%s needs package 'Matrix' correctly installed",@\\
\mbox{}\verb@                              "free1way"),@\\
\mbox{}\verb@                     domain = NA)@\\
\mbox{}\verb@        A <- Matrix::bandSparse(length(Adiag), k = 0:1, diagonals = list(Adiag, Aoffdiag), @\\
\mbox{}\verb@                                symmetric = TRUE)@\\
\mbox{}\verb@    } else {@\\
\mbox{}\verb@        A <- matrix(Adiag)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    return(list(A = A, X = X, Z = Z))@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
We start with an example involving $K = 3$ groups for a binary outcome and
use a binary logistic regression model to estimate the two log-odds ratios
$\delta_2$ and $\delta_3$ along with their estimated covariance
<<glm>>=
library("free1way")
(x <- matrix(c(10, 5, 7, 11, 8, 9), nrow = 2))
d <- expand.grid(y = relevel(gl(2, 1), "2"), t = gl(3, 1))
d$x <- c(x)
m <- glm(y ~ t, data = d, weights = x, family = binomial())
(cf <- coef(m))
@

Replicating these results requires specification of the inverse link
function $F = \text{expit}$ and the density function $f$ of the standard
logistic. Note that \code{glm} operates with a positive linear predictor, so
we need to change the sign of the log-odds ratios:

<<glm-op>>=
F <- plogis
f <- dlogis
op <- optim(par = c("mt2" = 0, "mt3" = 0, "(Intercept)" = 0), 
            fn = .nll, gr = .nsc, 
            x = x, method = "BFGS", hessian = TRUE)
cbind(c(cf[-1] * -1, cf[1]), op$par)
logLik(m)
-op$value
@

Parameter estimates and the in-sample log-likelihood are practically
identical. We now turn to the inverse Hessian of the shift terms, first
defining the derivative of the density of the standard logistic distribtion
<<glm-H>>=
fp <- function(x) {
    p <- plogis(x)
    p * (1 - p)^2 - p^2 * (1 - p)
}
H <- .hes(op$par, x)
### analytical covariance of parameters
solve(H$Z - crossprod(H$X, Matrix::solve(H$A, H$X)))
### numerical covariance
solve(op$hessian)[1:2,1:2]
### from glm
vcov(m)[-1,-1]
@
Also here we see practically identical results. We will later implement a
low-level function \code{.free1way} taking a table and an object describing the inverse link
$F$ as arguments; these results are also in line with \code{glm}:
<<glm-free1way>>=
obj <- .free1wayML(as.table(x), link = logit())
obj$coefficients
-obj$value
### analytical covariance
obj$vcov
@

In the next step, we extend our results to the stratified case. We iterate
over all blocks and evaluate the negative log-likelihood for the same values
of the shift parameters but block-specific values of the intercept
parameters. Before we begin, we convert the table $C \times K \times B
(\times 2)$ table \code{x} into a list of non-empty $C^\prime \times K$
tables with non-zero row sums:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap12}\raggedright\small
\NWtarget{nuweb9}{} $\langle\,${\itshape table2list body}\nobreak\ {\footnotesize {9}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@dx <- dim(x)@\\
\mbox{}\verb@if (length(dx) == 1L)@\\
\mbox{}\verb@    stop("")@\\
\mbox{}\verb@if (length(dx) == 2L)@\\
\mbox{}\verb@    x <- as.table(array(x, dim = c(dx, 1)))@\\
\mbox{}\verb@ms <- c(list(x), lapply(seq_along(dx), function(j) marginSums(x, j) > 0))@\\
\mbox{}\verb@ms$drop <- FALSE@\\
\mbox{}\verb@x <- do.call("[", ms)@\\
\mbox{}\verb@dx <- dim(x)@\\
\mbox{}\verb@stopifnot(length(dx) >= 3L)@\\
\mbox{}\verb@C <- dim(x)[1L]@\\
\mbox{}\verb@K <- dim(x)[2L]@\\
\mbox{}\verb@B <- dim(x)[3L]@\\
\mbox{}\verb@stopifnot(dx[1L] > 1L)@\\
\mbox{}\verb@stopifnot(K > 1L)@\\
\mbox{}\verb@xrc <- NULL@\\
\mbox{}\verb@if (length(dx) == 4L) {@\\
\mbox{}\verb@    if (dx[4] == 2L) {@\\
\mbox{}\verb@        xrc <- array(x[,,,"FALSE", drop = TRUE], dim = dx[1:3])@\\
\mbox{}\verb@        x <- array(x[,,,"TRUE", drop = TRUE], dim = dx[1:3])@\\
\mbox{}\verb@    } else {@\\
\mbox{}\verb@        stop("")@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@\\
\mbox{}\verb@xlist <- xrclist <- vector(mode = "list", length = B)@\\
\mbox{}\verb@@\\
\mbox{}\verb@for (b in seq_len(B)) {@\\
\mbox{}\verb@    xb <- matrix(x[,,b, drop = TRUE], ncol = K)@\\
\mbox{}\verb@    xw <- rowSums(abs(xb)) > 0@\\
\mbox{}\verb@    ### do not remove last parameter if there are corresponding@\\
\mbox{}\verb@    ### right-censored observations@\\
\mbox{}\verb@    if (!is.null(xrc) && any(xrc[dx[1],,b,drop = TRUE] > 0))@\\
\mbox{}\verb@        xw[length(xw)] <- TRUE@\\
\mbox{}\verb@    if (sum(xw) > 1L) {@\\
\mbox{}\verb@        xlist[[b]] <- xb[xw,,drop = FALSE]@\\
\mbox{}\verb@        attr(xlist[[b]], "idx") <- xw@\\
\mbox{}\verb@        if (!is.null(xrc)) {@\\
\mbox{}\verb@            xrclist[[b]] <- matrix(xrc[xw,,b,drop = TRUE], ncol = K)@\\
\mbox{}\verb@            attr(xrclist[[b]], "idx") <- xw@\\
\mbox{}\verb@        }@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@}@\\
\mbox{}\verb@strata <- !sapply(xlist, is.null)@\\
\mbox{}\verb@xlist <- xlist[strata]@\\
\mbox{}\verb@xrclist <- xrclist[strata]@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb10a}{10a}\NWlink{nuweb20b}{, 20b}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap13}\raggedright\small
\NWtarget{nuweb10a}{} $\langle\,${\itshape table2list}\nobreak\ {\footnotesize {10a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.table2list <- function(x) {@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape table2list body}\nobreak\ {\footnotesize \NWlink{nuweb9}{9}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ret <- list(xlist = xlist)@\\
\mbox{}\verb@    if (!is.null(xrc))@\\
\mbox{}\verb@        ret$xrclist <- xrclist@\\
\mbox{}\verb@    ret$strata <- strata@\\
\mbox{}\verb@    ret@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item {\NWtxtMacroNoRef}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
We first extract the shift parameters $\delta_{\cdot}$ and then, separately
for each stratum, the corresponding contrasts of the intercept parameters:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap14}\raggedright\small
\NWtarget{nuweb10b}{} $\langle\,${\itshape stratum prep}\nobreak\ {\footnotesize {10b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@C <- sapply(x, NROW) ### might differ by stratum@\\
\mbox{}\verb@K <- unique(do.call("c", lapply(x, ncol))) ### the same@\\
\mbox{}\verb@B <- length(x)@\\
\mbox{}\verb@sidx <- factor(rep(seq_len(B), times = pmax(0, C - 1L)), levels = seq_len(B))@\\
\mbox{}\verb@bidx <- seq_len(K - 1L)@\\
\mbox{}\verb@delta <- parm[bidx]@\\
\mbox{}\verb@intercepts <- split(parm[-bidx], sidx)@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb10c}{10c}\NWlink{nuweb11a}{, 11a}\NWlink{nuweb11b}{b}\NWlink{nuweb12}{, 12}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
before we loop over the non-empty strata and return the sum of the
corresponding log-likelihoods:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap15}\raggedright\small
\NWtarget{nuweb10c}{} $\langle\,${\itshape stratified negative logLik}\nobreak\ {\footnotesize {10c}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.snll <- function(parm, x, mu = 0, rightcensored = FALSE) {@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape stratum prep}\nobreak\ {\footnotesize \NWlink{nuweb10b}{10b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    ret <- 0@\\
\mbox{}\verb@    for (b in seq_len(B))@\\
\mbox{}\verb@        ret <- ret + .nll(c(delta, intercepts[[b]]), x[[b]], mu = mu,@\\
\mbox{}\verb@                          rightcensored = rightcensored)@\\
\mbox{}\verb@    return(ret)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
In a similar way, we evaluate the gradients for each block and sum-up the
contributions by the shift parameters whereas the gradients for the
intercept parameters are only concatenated:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap16}\raggedright\small
\NWtarget{nuweb11a}{} $\langle\,${\itshape stratified negative score}\nobreak\ {\footnotesize {11a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.snsc <- function(parm, x, mu = 0, rightcensored = FALSE) {@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape stratum prep}\nobreak\ {\footnotesize \NWlink{nuweb10b}{10b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    ret <- numeric(length(bidx))@\\
\mbox{}\verb@    for (b in seq_len(B)) {@\\
\mbox{}\verb@        nsc <- .nsc(c(delta, intercepts[[b]]), x[[b]], mu = mu,@\\
\mbox{}\verb@                    rightcensored = rightcensored)@\\
\mbox{}\verb@        ret[bidx] <- ret[bidx] + nsc[bidx]@\\
\mbox{}\verb@        ret <- c(ret, nsc[-bidx])@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    return(ret)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The score residuum is zero for an observation with weight zero, that is, a
row of zeros in the table:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap17}\raggedright\small
\NWtarget{nuweb11b}{} $\langle\,${\itshape stratified negative score residual}\nobreak\ {\footnotesize {11b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.snsr <- function(parm, x, mu = 0, rightcensored = FALSE) {@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape stratum prep}\nobreak\ {\footnotesize \NWlink{nuweb10b}{10b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    ret <- c()@\\
\mbox{}\verb@    for (b in seq_len(B)) {@\\
\mbox{}\verb@        idx <- attr(x[[b]], "idx")@\\
\mbox{}\verb@        sr <- numeric(length(idx))@\\
\mbox{}\verb@        sr[idx] <- .nsr(c(delta, intercepts[[b]]), x[[b]], mu = mu,@\\
\mbox{}\verb@                        rightcensored = rightcensored)@\\
\mbox{}\verb@        ret <- c(ret, sr)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    return(ret)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
<<glm-stratum>>=
(x <- as.table(array(c(10, 5, 7, 11, 8, 9,
                        9, 4, 8, 15, 5, 4), dim = c(2, 3, 2))))
d <- expand.grid(y = relevel(gl(2, 1), "2"), t = gl(3, 1), s = gl(2, 1))
d$x <- c(x)
m <- glm(y ~ 0 + s + t, data = d, weights = x, family = binomial())
logLik(m)
(cf <- coef(m))
@

<<glm-op-stratum>>=
xl <- .table2list(x)$xlist
op <- optim(par = c("mt2" = 0, "mt3" = 0, "(Intercept 1)" = 0, "(Intercept 2)" = 0), 
            fn = .snll, gr = .snsc, 
            x = xl, 
            method = "BFGS", 
            hessian = TRUE)
cbind(c(cf[-(1:2)] * -1, cf[1:2]), op$par)
logLik(m)
-op$value
@

For the analytical Hessian, we sum-up over the stratum-specific
Hessians of the shift parameters. For right-censored observations, we need
to compute the contributions by the events and obtain the joint Hessian for
shift- and intercept parameters first:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap18}\raggedright\small
\NWtarget{nuweb12}{} $\langle\,${\itshape stratified Hessian}\nobreak\ {\footnotesize {12}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.shes <- function(parm, x, mu = 0, xrc = NULL) {@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape stratum prep}\nobreak\ {\footnotesize \NWlink{nuweb10b}{10b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    ret <- matrix(0, nrow = length(bidx), ncol = length(bidx))@\\
\mbox{}\verb@    for (b in seq_len(B)) {@\\
\mbox{}\verb@        H <- .hes(c(delta, intercepts[[b]]), x[[b]], mu = mu)@\\
\mbox{}\verb@        if (!is.null(xrc)) {@\\
\mbox{}\verb@            Hrc <- .hes(c(delta, intercepts[[b]]), xrc[[b]], mu = mu, @\\
\mbox{}\verb@                        rightcensored = TRUE)@\\
\mbox{}\verb@            H$X <- H$X + Hrc$X@\\
\mbox{}\verb@            H$A <- H$A + Hrc$A@\\
\mbox{}\verb@            H$Z <- H$Z + Hrc$Z@\\
\mbox{}\verb@        }@\\
\mbox{}\verb@        sAH <- try(Matrix::solve(H$A, H$X))@\\
\mbox{}\verb@        if (inherits(sAH, "try-error"))@\\
\mbox{}\verb@            stop("Error computing the Hessian in free1way")@\\
\mbox{}\verb@        ret <- ret + (H$Z - crossprod(H$X, sAH))@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    as.matrix(ret)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
<<glm-H-stratum>>=
### analytical covariance of parameters
solve(.shes(op$par, xl))
### numerical covariance
solve(op$hessian)[1:2,1:2]
### from glm
vcov(m)[-(1:2),-(1:2)]
@

<<glm-free1way-strata>>=
obj <- .free1wayML(as.table(x), link = logit())
obj$coefficients
-obj$value
### analytical covariance
obj$vcov
@
	

\chapter{Link Functions}
\label{ch:link}

Similar to \code{family} objects, we provide some infrastructure for
\code{link} functions $F^{-1}$ and derived quantities (\code{linkinv} $F$,
\code{dlinkinv} $f$, and \code{ddlinkinv} $f^\prime$). If not provided, we also
set-up the ratio $f^\prime / f$ in the constructor.

Although there is some overlap with \code{family} objects for binomial
outcomes, it doesn't seem beneficial to extend this richer class.

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap19}\raggedright\small
\NWtarget{nuweb14}{} \verb@"linkfun.R"@\nobreak\ {\footnotesize {14}}$\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape linkfun}\nobreak\ {\footnotesize \NWlink{nuweb15}{15}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape logit}\nobreak\ {\footnotesize \NWlink{nuweb16}{16}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape probit}\nobreak\ {\footnotesize \NWlink{nuweb19}{19}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape cloglog}\nobreak\ {\footnotesize \NWlink{nuweb18}{18}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape loglog}\nobreak\ {\footnotesize \NWlink{nuweb17}{17}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap20}\raggedright\small
\NWtarget{nuweb15}{} $\langle\,${\itshape linkfun}\nobreak\ {\footnotesize {15}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.p <- function(link, q, ...)@\\
\mbox{}\verb@    link$linkinv(q = q, ...)@\\
\mbox{}\verb@@\\
\mbox{}\verb@.q <- function(link, p, ...)@\\
\mbox{}\verb@    link$link(p = p, ...)@\\
\mbox{}\verb@@\\
\mbox{}\verb@.d <- function(link, x, ...)@\\
\mbox{}\verb@    link$dlinkinv(x = x, ...)@\\
\mbox{}\verb@@\\
\mbox{}\verb@.dd <- function(link, x, ...)@\\
\mbox{}\verb@    link$ddlinkinv(x = x, ...)@\\
\mbox{}\verb@@\\
\mbox{}\verb@.ddd <- function(link, x, ...)@\\
\mbox{}\verb@    link$dddlinkinv(x = x, ...)@\\
\mbox{}\verb@@\\
\mbox{}\verb@.dd2d <- function(link, x, ...)@\\
\mbox{}\verb@    link$dd2dlinkinv(x = x, ...)@\\
\mbox{}\verb@@\\
\mbox{}\verb@linkfun <- function(alias, @\\
\mbox{}\verb@                    model, @\\
\mbox{}\verb@                    parm, @\\
\mbox{}\verb@                    link, @\\
\mbox{}\verb@                    linkinv,@\\
\mbox{}\verb@                    dlinkinv, @\\
\mbox{}\verb@                    ddlinkinv,@\\
\mbox{}\verb@                    ...) {@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ret <- list(alias = alias,@\\
\mbox{}\verb@                model = model,@\\
\mbox{}\verb@                parm = parm,@\\
\mbox{}\verb@                link = link,@\\
\mbox{}\verb@                linkinv = linkinv,@\\
\mbox{}\verb@                dlinkinv = dlinkinv,@\\
\mbox{}\verb@                ddlinkinv = ddlinkinv)@\\
\mbox{}\verb@    if (is.null(ret$dd2d)) @\\
\mbox{}\verb@        ret$dd2d <- function(x) @\\
\mbox{}\verb@            ret$ddlinkinv(x) / ret$dlinkinv(x)@\\
\mbox{}\verb@    ret <- c(ret, list(...))@\\
\mbox{}\verb@    class(ret) <- "linkfun"@\\
\mbox{}\verb@    ret@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb14}{14}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
We start with the logit link, that is $F(z) = (1 + \exp(-z))^{-1}$, giving rise
to Wilcoxon or Kruskal-Wallis type score residuals:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap21}\raggedright\small
\NWtarget{nuweb16}{} $\langle\,${\itshape logit}\nobreak\ {\footnotesize {16}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@logit <- function()@\\
\mbox{}\verb@    linkfun(alias = c("Wilcoxon", "Kruskal-Wallis"),@\\
\mbox{}\verb@            model = "proportional odds", @\\
\mbox{}\verb@            parm = "log-odds ratio",@\\
\mbox{}\verb@            link = qlogis,@\\
\mbox{}\verb@            linkinv = plogis,@\\
\mbox{}\verb@            dlinkinv = dlogis,@\\
\mbox{}\verb@            ddlinkinv = function(x) {@\\
\mbox{}\verb@                p <- plogis(x)@\\
\mbox{}\verb@                p * (1 - p)^2 - p^2 * (1 - p)@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            dddlinkinv = function(x) {@\\
\mbox{}\verb@                ex <- exp(x)@\\
\mbox{}\verb@                ifelse(is.finite(x), (ex - 4 * ex^2 + ex^3) / (1 + ex)^4, 0.0)@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            dd2d = function(x) {@\\
\mbox{}\verb@                ex <- exp(x)@\\
\mbox{}\verb@                (1 - ex) / (1 + ex)@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            parm2PI = function(x) {@\\
\mbox{}\verb@               OR <- exp(x)@\\
\mbox{}\verb@               ret <- OR * (OR - 1 - x)/(OR - 1)^2@\\
\mbox{}\verb@               ret[abs(x) < .Machine$double.eps] <- 0.5@\\
\mbox{}\verb@               return(ret)@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            PI2parm = function(p) {@\\
\mbox{}\verb@               f <- function(x, PI)@\\
\mbox{}\verb@                   x + (exp(-x) * (PI + exp(2 * x) * (PI - 1) + exp(x)* (1 - 2 * PI)))@\\
\mbox{}\verb@               ret <- sapply(p, function(p) @\\
\mbox{}\verb@                   uniroot(f, PI = p, interval = 50 * c(-1, 1))$root)@\\
\mbox{}\verb@               return(ret)@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            parm2OVL = function(x) 2 * plogis(-abs(x / 2))@\\
\mbox{}\verb@    )@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb14}{14}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The \code{parm2PI} function converts log-odds ratios to probabilistic
indices (or AUCs) and the inverse operation is implemented by
\code{PI2parm}. The overlap coefficient can be obtained from a log-odds
ratio via \code{parm2OVL}.

The log-log link, with $F(z) = \exp(-\exp(-z))$, is used to construct tests
against Lehmann alternatives:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap22}\raggedright\small
\NWtarget{nuweb17}{} $\langle\,${\itshape loglog}\nobreak\ {\footnotesize {17}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@loglog <- function()@\\
\mbox{}\verb@    linkfun(alias = "Lehmann", @\\
\mbox{}\verb@            model = "Lehmann", @\\
\mbox{}\verb@            parm = "log-reverse time hazard ratio",@\\
\mbox{}\verb@            link = function(p, log.p = FALSE) {@\\
\mbox{}\verb@                if (!log.p) p <- log(p)@\\
\mbox{}\verb@                -log(-p)@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            linkinv = function(q, lower.tail = TRUE, log.p = FALSE) {@\\
\mbox{}\verb@                ### p = exp(-exp(-q))@\\
\mbox{}\verb@                if (log.p) {@\\
\mbox{}\verb@                    if (lower.tail)@\\
\mbox{}\verb@                        return(-exp(-q))@\\
\mbox{}\verb@                    return(log1p(-exp(-exp(-q))))@\\
\mbox{}\verb@                }@\\
\mbox{}\verb@                if (lower.tail)@\\
\mbox{}\verb@                    return(exp(-exp(-q)))@\\
\mbox{}\verb@                -expm1(-exp(-q))@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            dlinkinv = function(x) @\\
\mbox{}\verb@                ifelse(is.finite(x), exp(- x - exp(-x)), 0.0),@\\
\mbox{}\verb@            ddlinkinv = function(x) {@\\
\mbox{}\verb@               ex <- exp(-x)@\\
\mbox{}\verb@               ifelse(is.finite(x), exp(-ex - x) * (ex - 1.0), 0.0)@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            dddlinkinv = function(x) {@\\
\mbox{}\verb@               ex <- exp(-x)@\\
\mbox{}\verb@               ifelse(is.finite(x), exp(-x - ex) * (ex - 1)^2 - exp(-ex - 2 * x), 0.0)@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            dd2d = function(x) @\\
\mbox{}\verb@                expm1(-x),@\\
\mbox{}\verb@            parm2PI = plogis,@\\
\mbox{}\verb@            PI2parm = qlogis,@\\
\mbox{}\verb@            parm2OVL = function(x) {@\\
\mbox{}\verb@                x <- abs(x)@\\
\mbox{}\verb@                rt <- exp(-x / (exp(x) - 1))@\\
\mbox{}\verb@                ret <- rt^exp(x) + 1 - rt@\\
\mbox{}\verb@                ret[abs(x) < .Machine$double.eps] <- 1@\\
\mbox{}\verb@                x[] <- ret@\\
\mbox{}\verb@                return(x)@\\
\mbox{}\verb@            }@\\
\mbox{}\verb@    )@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb14}{14}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The complementary log-log link, with $F(z) = 1 - \exp(-\exp(z))$, provides
log-rank or Savage score residuals against proportional hazards
alternatives:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap23}\raggedright\small
\NWtarget{nuweb18}{} $\langle\,${\itshape cloglog}\nobreak\ {\footnotesize {18}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@cloglog <- function()@\\
\mbox{}\verb@    linkfun(alias = "Savage",@\\
\mbox{}\verb@            model = "proportional hazards", @\\
\mbox{}\verb@            parm = "log-hazard ratio",@\\
\mbox{}\verb@            link = function(p, log.p = FALSE) {@\\
\mbox{}\verb@                if (log.p) p <- exp(p)@\\
\mbox{}\verb@                log(-log1p(- p))@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            linkinv = function(q, lower.tail = TRUE, log.p = FALSE) {@\\
\mbox{}\verb@                ### p = 1 - exp(-exp(q))@\\
\mbox{}\verb@                ret <- exp(-exp(q))@\\
\mbox{}\verb@                if (log.p) {@\\
\mbox{}\verb@                    if (lower.tail)@\\
\mbox{}\verb@                        return(log1p(-ret))@\\
\mbox{}\verb@                    return(-exp(q))@\\
\mbox{}\verb@                }@\\
\mbox{}\verb@                if (lower.tail)@\\
\mbox{}\verb@                    return(-expm1(-exp(q)))@\\
\mbox{}\verb@                return(ret)@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            dlinkinv = function(x) @\\
\mbox{}\verb@                ifelse(is.finite(x), exp(x - exp(x)), 0.0),@\\
\mbox{}\verb@            ddlinkinv = function(x) {@\\
\mbox{}\verb@                ex <- exp(x)@\\
\mbox{}\verb@                ifelse(is.finite(x), (ex - ex^2) / exp(ex), 0.0)@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            dddlinkinv = function(x) {@\\
\mbox{}\verb@                ex <- exp(x)@\\
\mbox{}\verb@                ifelse(is.finite(x), (ex - 3*ex^2 + ex^3) / exp(ex), 0.0)@\\
\mbox{}\verb@            },@\\
\mbox{}\verb@            dd2d = function(x)@\\
\mbox{}\verb@               -expm1(x),@\\
\mbox{}\verb@            parm2PI = plogis,@\\
\mbox{}\verb@            PI2parm = qlogis,@\\
\mbox{}\verb@            parm2OVL = function(x) {@\\
\mbox{}\verb@                x <- abs(x)@\\
\mbox{}\verb@                ret <- exp(x / (exp(-x) - 1)) - exp(-x / (exp(x) - 1)) + 1 @\\
\mbox{}\verb@                ret[abs(x) < .Machine$double.eps] <- 1@\\
\mbox{}\verb@                x[] <- ret@\\
\mbox{}\verb@                return(x)@\\
\mbox{}\verb@            }@\\
\mbox{}\verb@    )@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb14}{14}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The probit link, with $F(z) = \Phi$, leads to normal scores tests, where the
shift effect can be interpreted as a generalised version of Cohen's $d$,
that is, differences on a latent normal scale with variance one:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap24}\raggedright\small
\NWtarget{nuweb19}{} $\langle\,${\itshape probit}\nobreak\ {\footnotesize {19}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@probit <- function()@\\
\mbox{}\verb@    linkfun(alias = "van der Waerden normal scores",@\\
\mbox{}\verb@            model = "latent normal shift", @\\
\mbox{}\verb@            parm = "generalised Cohen's d",@\\
\mbox{}\verb@            link = qnorm,@\\
\mbox{}\verb@            linkinv = pnorm,@\\
\mbox{}\verb@            dlinkinv = dnorm,@\\
\mbox{}\verb@            ddlinkinv = function(x) @\\
\mbox{}\verb@                ifelse(is.finite(x), -dnorm(x = x) * x, 0.0), @\\
\mbox{}\verb@            dddlinkinv = function(x) @\\
\mbox{}\verb@                ifelse(is.finite(x), dnorm(x = x) * (x^2 - 1), 0.0),@\\
\mbox{}\verb@            dd2d = function(x) -x,@\\
\mbox{}\verb@            parm2PI = function(x) pnorm(x, sd = sqrt(2)),@\\
\mbox{}\verb@            PI2parm = function(p) qnorm(p, sd = sqrt(2)),@\\
\mbox{}\verb@            parm2OVL = function(x) 2 * pnorm(-abs(x / 2))@\\
\mbox{}\verb@    )@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb14}{14}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\chapter{ML Estimation}
\label{ch:ML}

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap25}\raggedright\small
\NWtarget{nuweb20a}{} \verb@"free1way.R"@\nobreak\ {\footnotesize {20a}}$\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape ML estimation}\nobreak\ {\footnotesize \NWlink{nuweb25}{25}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape free1way}\nobreak\ {\footnotesize \NWlink{nuweb34}{34}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape free1way methods}\nobreak\ {\footnotesize \NWlink{nuweb39}{39}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape free1way print}\nobreak\ {\footnotesize \NWlink{nuweb40}{40}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape free1way summary}\nobreak\ {\footnotesize \NWlink{nuweb41}{41}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape free1way confint}\nobreak\ {\footnotesize \NWlink{nuweb42}{42}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape free1way formula}\nobreak\ {\footnotesize \NWlink{nuweb36}{36}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape free1way numeric}\nobreak\ {\footnotesize \NWlink{nuweb37b}{37b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape free1way factor}\nobreak\ {\footnotesize \NWlink{nuweb38}{38}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape ppplot}\nobreak\ {\footnotesize \NWlink{nuweb58}{58}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape rfree1way}\nobreak\ {\footnotesize \NWlink{nuweb65}{65}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape power}\nobreak\ {\footnotesize \NWlink{nuweb?}{?}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
We now put together a low-level function for parameter estimation and
evaluation of scores, Hessians, and residuals. We also set-up a profile
likelihood function for later re-use. 

Assuming all shift effects been zero, we compute starting values for the
intercept parameters from the empirical cumulative distribution function
after merging all treatment groups:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap26}\raggedright\small
\NWtarget{nuweb20b}{} $\langle\,${\itshape setup and starting values}\nobreak\ {\footnotesize {20b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape table2list body}\nobreak\ {\footnotesize \NWlink{nuweb9}{9}}$\,\rangle$}\verb@@\\
\mbox{}\verb@if (NS <- is.null(start))@\\
\mbox{}\verb@    start <- rep.int(0, K - 1)@\\
\mbox{}\verb@lwr <- rep(-Inf, times = K - 1)@\\
\mbox{}\verb@for (b in seq_len(length(xlist))) {@\\
\mbox{}\verb@    bC <- nrow(xlist[[b]]) - 1L@\\
\mbox{}\verb@    lwr <- c(lwr, -Inf, rep.int(0, times = bC - 1L))@\\
\mbox{}\verb@    if (NS) {@\\
\mbox{}\verb@        ecdf0 <- cumsum(rowSums(xlist[[b]]))@\\
\mbox{}\verb@        ecdf0 <- ecdf0[-length(ecdf0)] / ecdf0[length(ecdf0)]@\\
\mbox{}\verb@        Qecdf <- Q(ecdf0)@\\
\mbox{}\verb@        bstart <- log(diff(Qecdf))@\\
\mbox{}\verb@        start <- c(start, Qecdf[1], bstart)@\\
\mbox{}\verb@        start[!is.finite(start)] <- 0@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The profile negative log-likelihood can be evaluated for some of the
parameters in $\thetavec$ (denoted as \code{fix}), the remaining parameters
are updated. Note that \code{start} must contain the full parameter vector
$\thetavec$.

We call \code{optim} and will increase the \code{maxit} control parameter if
we encounter optimisation issues and restart at the current solution:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap27}\raggedright\small
\NWtarget{nuweb21}{} $\langle\,${\itshape do optim}\nobreak\ {\footnotesize {21}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@maxit <- 100@\\
\mbox{}\verb@while(maxit < 10001) {@\\
\mbox{}\verb@   ret <- do.call("optim", opargs)@\\
\mbox{}\verb@   maxit <- 5 * maxit@\\
\mbox{}\verb@   if (ret$convergence > 0) {@\\
\mbox{}\verb@       if (is.null(opargs$control))@\\
\mbox{}\verb@           opargs$control <- list(maxit = maxit)@\\
\mbox{}\verb@        else @\\
\mbox{}\verb@           opargs$control$maxit <- maxit@\\
\mbox{}\verb@       opargs$par <- ret$par@\\
\mbox{}\verb@   } else {@\\
\mbox{}\verb@       break()@\\
\mbox{}\verb@   }@\\
\mbox{}\verb@}@\\
\mbox{}\verb@if (ret$convergence > 0)@\\
\mbox{}\verb@    stop(paste("Unsuccessful optimisation in free1way", ret$message))@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb22}{22}\NWlink{nuweb23}{, 23}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
We first set-up the target function (the negative log-likelihood, also
dealing with right-censoring) and the corresponding gradient. We then add
the profile negative log-likelihood, which in turn calls the two functions
defined first

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap28}\raggedright\small
\NWtarget{nuweb22}{} $\langle\,${\itshape profile}\nobreak\ {\footnotesize {22}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@fn <- function(par) {@\\
\mbox{}\verb@    par[is.finite(lwr)] <- exp(par[is.finite(lwr)])@\\
\mbox{}\verb@    ret <- .snll(par, x = xlist, mu = mu)@\\
\mbox{}\verb@    if (!is.null(xrc))@\\
\mbox{}\verb@        ret <- ret + .snll(par, x = xrclist, mu = mu, @\\
\mbox{}\verb@                           rightcensored = TRUE)@\\
\mbox{}\verb@    return(ret)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@gr <- function(par) {@\\
\mbox{}\verb@    par[is.finite(lwr)] <- exp(par[is.finite(lwr)])@\\
\mbox{}\verb@    ret <- .snsc(par, x = xlist, mu = mu)@\\
\mbox{}\verb@    if (!is.null(xrc))@\\
\mbox{}\verb@        ret <- ret + .snsc(par, x = xrclist, mu = mu, @\\
\mbox{}\verb@                           rightcensored = TRUE)@\\
\mbox{}\verb@    par[!is.finite(lwr)] <- 1@\\
\mbox{}\verb@    return(ret * par)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@.profile <- function(start, fix = seq_len(K - 1)) {@\\
\mbox{}\verb@    stopifnot(all(fix %in% seq_len(K - 1)))@\\
\mbox{}\verb@    delta <- start[fix]@\\
\mbox{}\verb@    opargs <- c(list(par = start[-fix], @\\
\mbox{}\verb@                     fn = function(par) {@\\
\mbox{}\verb@                         p <- numeric(length(par) + length(fix))@\\
\mbox{}\verb@                         p[fix] <- delta@\\
\mbox{}\verb@                         p[-fix] <- par@\\
\mbox{}\verb@                         fn(p)@\\
\mbox{}\verb@                     },@\\
\mbox{}\verb@                     gr = function(par) {@\\
\mbox{}\verb@                         p <- numeric(length(par) + length(fix))@\\
\mbox{}\verb@                         p[fix] <- delta@\\
\mbox{}\verb@                         p[-fix] <- par@\\
\mbox{}\verb@                         gr(p)[-fix]@\\
\mbox{}\verb@                     },@\\
\mbox{}\verb@                     method = "BFGS", @\\
\mbox{}\verb@                     hessian = FALSE),@\\
\mbox{}\verb@                     list(...))@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape do optim}\nobreak\ {\footnotesize \NWlink{nuweb21}{21}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    p <- numeric(length(start))@\\
\mbox{}\verb@    p[fix] <- delta@\\
\mbox{}\verb@    p[-fix] <- ret$par@\\
\mbox{}\verb@    ret$par <- p@\\
\mbox{}\verb@    ret@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The heart of the function is a call to \code{optim}, trying to obtain
parameter estimates of $\thetavec$ by minimising the negative
log-likelihood. We allow some (or all) parameters to be fixed at some
constants, and provide a profile version of the likelihood:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap29}\raggedright\small
\NWtarget{nuweb23}{} $\langle\,${\itshape optim}\nobreak\ {\footnotesize {23}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@if (!length(fix)) {@\\
\mbox{}\verb@    opargs <- c(list(par = start, fn = fn, gr = gr,@\\
\mbox{}\verb@                     # lower = lwr, @\\
\mbox{}\verb@                     method = "BFGS", @\\
\mbox{}\verb@                     hessian = FALSE), list(...))@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape do optim}\nobreak\ {\footnotesize \NWlink{nuweb21}{21}}$\,\rangle$}\verb@@\\
\mbox{}\verb@} else if (length(fix) == length(start)) {@\\
\mbox{}\verb@    ret <- list(par = start, @\\
\mbox{}\verb@                value = fn(start))@\\
\mbox{}\verb@} else {@\\
\mbox{}\verb@    ret <- .profile(start, fix = fix)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
After parameter estimation, we evaluate negative scores, the Hessian, and
negative residuals as requested:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap30}\raggedright\small
\NWtarget{nuweb24}{} $\langle\,${\itshape post processing}\nobreak\ {\footnotesize {24}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@if (is.null(fix) || (length(fix) == length(start)))@\\
\mbox{}\verb@    parm <- seq_len(K - 1)@\\
\mbox{}\verb@else @\\
\mbox{}\verb@    parm <- fix@\\
\mbox{}\verb@if (any(parm >= K)) return(ret)@\\
\mbox{}\verb@@\\
\mbox{}\verb@ret$coefficients <- ret$par[parm]@\\
\mbox{}\verb@dn2 <- dimnames(xt)[2L]@\\
\mbox{}\verb@names(ret$coefficients) <- cnames <- paste0(names(dn2), dn2[[1L]][1L + parm])@\\
\mbox{}\verb@@\\
\mbox{}\verb@par <- ret$par@\\
\mbox{}\verb@par[is.finite(lwr)] <- exp(par[is.finite(lwr)])@\\
\mbox{}\verb@@\\
\mbox{}\verb@if (score) {@\\
\mbox{}\verb@    ret$negscore <- .snsc(par, x = xlist, mu = mu)[parm]@\\
\mbox{}\verb@    if (!is.null(xrc))@\\
\mbox{}\verb@        ret$negscore <- ret$negscore + .snsc(par, x = xrclist, mu = mu, @\\
\mbox{}\verb@                                             rightcensored = TRUE)[parm]@\\
\mbox{}\verb@}@\\
\mbox{}\verb@if (hessian) {@\\
\mbox{}\verb@    if (!is.null(xrc)) {@\\
\mbox{}\verb@        ret$hessian <- .shes(par, x = xlist, mu = mu, xrc = xrclist)@\\
\mbox{}\verb@    } else {@\\
\mbox{}\verb@        ret$hessian <- .shes(par, x = xlist, mu = mu)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    ret$vcov <- solve(ret$hessian)@\\
\mbox{}\verb@    if (length(parm) != nrow(ret$hessian))@\\
\mbox{}\verb@       ret$hessian <- solve(ret$vcov <- ret$vcov[parm, parm, drop = FALSE])@\\
\mbox{}\verb@    rownames(ret$vcov) <- colnames(ret$vcov) <- rownames(ret$hessian) <-@\\
\mbox{}\verb@        colnames(ret$hessian) <-  cnames@\\
\mbox{}\verb@}@\\
\mbox{}\verb@if (residuals) {@\\
\mbox{}\verb@    ret$negresiduals <- .snsr(par, x = xlist, mu = mu)@\\
\mbox{}\verb@    if (!is.null(xrc)) {@\\
\mbox{}\verb@        rcr <- .snsr(par, x = xrclist, mu = mu, rightcensored = TRUE)@\\
\mbox{}\verb@        ret$negresiduals <- c(rbind(matrix(ret$negresiduals, nrow = C),@\\
\mbox{}\verb@                                    matrix(rcr, nrow = C)))@\\
\mbox{}\verb@     }@\\
\mbox{}\verb@}@\\
\mbox{}\verb@ret$profile <- function(start, fix)@\\
\mbox{}\verb@    .free1wayML(xt, link = link, mu = mu, start = start, fix = fix, tol = tol, @\\
\mbox{}\verb@               ...) @\\
\mbox{}\verb@ret$table <- xt@\\
\mbox{}\verb@ret$mu <- mu@\\
\mbox{}\verb@ret$strata <- strata@\\
\mbox{}\verb@names(ret$mu) <- link$parm@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb25}{25}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
Finally, we put everything into one function which returns an object of
class \code{free1wayML} for later use:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap31}\raggedright\small
\NWtarget{nuweb25}{} $\langle\,${\itshape ML estimation}\nobreak\ {\footnotesize {25}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.free1wayML <- function(x, link, mu = 0, start = NULL, fix = NULL, @\\
\mbox{}\verb@                        residuals = TRUE, score = TRUE, hessian = TRUE, @\\
\mbox{}\verb@                        tol = sqrt(.Machine$double.eps), ...) {@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ### convert to three-way table@\\
\mbox{}\verb@    xt <- x@\\
\mbox{}\verb@    stopifnot(is.table(x))@\\
\mbox{}\verb@    dx <- dim(x)@\\
\mbox{}\verb@    dn <- dimnames(x)@\\
\mbox{}\verb@    if (length(dx) == 2L) {@\\
\mbox{}\verb@        x <- as.table(array(c(x), dim = dx <- c(dx, 1L)))@\\
\mbox{}\verb@        dimnames(x) <- dn <- c(dn, list(A = "A"))@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ### short-cuts for link functions@\\
\mbox{}\verb@    F <- function(q) .p(link, q = q)@\\
\mbox{}\verb@    Q <- function(p) .q(link, p = p)@\\
\mbox{}\verb@    f <- function(q) .d(link, x = q)@\\
\mbox{}\verb@    fp <- function(q) .dd(link, x = q)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape setup and starting values}\nobreak\ {\footnotesize \NWlink{nuweb20b}{20b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape cumsumrev}\nobreak\ {\footnotesize \NWlink{nuweb4c}{4c}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape negative logLik}\nobreak\ {\footnotesize \NWlink{nuweb3b}{3b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape negative score}\nobreak\ {\footnotesize \NWlink{nuweb4b}{4b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape negative score residuals}\nobreak\ {\footnotesize \NWlink{nuweb5a}{5a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape Hessian}\nobreak\ {\footnotesize \NWlink{nuweb7}{7}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape stratified negative logLik}\nobreak\ {\footnotesize \NWlink{nuweb10c}{10c}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape stratified negative score}\nobreak\ {\footnotesize \NWlink{nuweb11a}{11a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape stratified Hessian}\nobreak\ {\footnotesize \NWlink{nuweb12}{12}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape stratified negative score residual}\nobreak\ {\footnotesize \NWlink{nuweb11b}{11b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape profile}\nobreak\ {\footnotesize \NWlink{nuweb22}{22}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape optim}\nobreak\ {\footnotesize \NWlink{nuweb23}{23}}$\,\rangle$}\verb@ @\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape post processing}\nobreak\ {\footnotesize \NWlink{nuweb24}{24}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    class(ret) <- "free1wayML"@\\
\mbox{}\verb@    ret@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb20a}{20a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
As an example, consider a stratified (two stata) $3 \times 3$ problem where
outcome category B is missing from the second stratum:

<<workhorse>>=
N <- 10
a <- matrix(c(5, 6, 4,
                    3, 5, 7,
                    3, 4, 5,
                    3, 5, 6,
                    0, 0, 0,
                    4, 6, 5), ncol = 3, byrow = TRUE)
x <- as.table(array(c(a[1:3,], a[-(1:3),]), dim = c(3, 3, 2)))
x
ret <- .free1wayML(x, logit())
ret[c("value", "par")]
cf <- ret$par
cf[1:2] <- cf[1:2] + .5
cf
### profile for cf[1:2]
.free1wayML(x, logit(), start = cf, fix = 1:2)[c("value", "par")]
### profile for cf[2]
.free1wayML(x, logit(), start = cf, fix = 2)[c("value", "par")]
### evaluate log-likelihood at cf
.free1wayML(x, logit(), start = cf, 
            fix = seq_along(ret$par))[c("value", "par")]
@

\chapter{ML Inference}
\label{ch:MLinf}

Based on an object of class \code{free1wayML}, we can setup different test
statistics and obtain the limiting null distribution based on classical ML
theory under the population model:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap32}\raggedright\small
\NWtarget{nuweb27a}{} $\langle\,${\itshape statistics}\nobreak\ {\footnotesize {27a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@if (test == "Wald") {@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape Wald statistic}\nobreak\ {\footnotesize \NWlink{nuweb27b}{27b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@} else if (test == "LRT") {@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape LRT}\nobreak\ {\footnotesize \NWlink{nuweb28a}{28a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@} else if (test == "Rao") {@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape Rao}\nobreak\ {\footnotesize \NWlink{nuweb28b}{28b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@} else if (test == "Permutation") {@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape Permutation}\nobreak\ {\footnotesize \NWlink{nuweb29}{29}}$\,\rangle$}\verb@@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb40}{40}\NWlink{nuweb42}{, 42}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\section{Wald Statistics}

We only need access to the parameter estimates $\hat{\delta}_2, \dots,
\hat{\delta}_K$ and the corresponding Hessian:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap33}\raggedright\small
\NWtarget{nuweb27b}{} $\langle\,${\itshape Wald statistic}\nobreak\ {\footnotesize {27b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@if (alternative == "two.sided") {@\\
\mbox{}\verb@    STATISTIC <- c("Wald chi-squared" = c(crossprod(cf, x$hessian %*% cf)))@\\
\mbox{}\verb@    DF <- c("df" = length(parm))@\\
\mbox{}\verb@    PVAL <- pchisq(STATISTIC, df = DF, lower.tail = FALSE)@\\
\mbox{}\verb@} else {@\\
\mbox{}\verb@    STATISTIC <- c("Wald Z" = unname(c(cf * sqrt(c(x$hessian)))))@\\
\mbox{}\verb@    PVAL <- pnorm(STATISTIC, lower.tail = alternative == "less")@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb27a}{27a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\section{Likelihood-ratio Statistics}

In addition to the log-likelihood evaluated at the ML estimates, we need to
evaluate the profile log-likelihood at some value corresponding the null
hypothesis to be tested:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap34}\raggedright\small
\NWtarget{nuweb28a}{} $\langle\,${\itshape LRT}\nobreak\ {\footnotesize {28a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@par <- x$par@\\
\mbox{}\verb@par[parm] <- value@\\
\mbox{}\verb@unll <- x$value ### neg logLik@\\
\mbox{}\verb@rnll <- x$profile(par, parm)$value ### neg logLik@\\
\mbox{}\verb@STATISTIC <- c("logLR chi-squared" = - 2 * (unll - rnll))@\\
\mbox{}\verb@DF <- c("df" = length(parm))@\\
\mbox{}\verb@PVAL <- pchisq(STATISTIC, df = DF, lower.tail = FALSE)@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb27a}{27a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\section{Rao Score Statistics}

For the Rao score test, the inverse of the Hessian as well as the score
function of the shift parameters evaluated for some null values need to be
computed:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap35}\raggedright\small
\NWtarget{nuweb28b}{} $\langle\,${\itshape Rao}\nobreak\ {\footnotesize {28b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@par <- x$par@\\
\mbox{}\verb@par[parm] <- value@\\
\mbox{}\verb@ret <- x$profile(par, parm)@\\
\mbox{}\verb@if (alternative == "two.sided") {@\\
\mbox{}\verb@    STATISTIC <- c("Rao chi-squared" = c(crossprod(ret$negscore, ret$vcov %*% ret$negscore)))@\\
\mbox{}\verb@    DF <- c("df" = length(parm))@\\
\mbox{}\verb@    PVAL <- pchisq(STATISTIC, df = DF, lower.tail = FALSE)@\\
\mbox{}\verb@} else {@\\
\mbox{}\verb@    STATISTIC <- c("Rao Z" = unname(- ret$negscore * sqrt(c(ret$vcov))))@\\
\mbox{}\verb@    PVAL <- pnorm(STATISTIC, lower.tail = alternative == "less")@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb27a}{27a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\chapter{Permutation Inference}
\label{ch:Perminf}

Under the permutation model, that is, in randomised experiments where the
random treatment allocation is the only relevant source of randomness, we
compute a permuation variant of the Rao score test, based on the conditional
asymptotic distribution or based on a Monte-Carlo estimate of the reference
distribution:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap36}\raggedright\small
\NWtarget{nuweb29}{} $\langle\,${\itshape Permutation}\nobreak\ {\footnotesize {29}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@par <- x$par@\\
\mbox{}\verb@par[parm] <- value@\\
\mbox{}\verb@ret <- x$profile(par, parm)@\\
\mbox{}\verb@sc <- - ret$negscore@\\
\mbox{}\verb@if (length(cf) == 1L)@\\
\mbox{}\verb@   sc <- sc / sqrt(c(ret$hessian))@\\
\mbox{}\verb@Esc <- sc - x$perm$Expectation@\\
\mbox{}\verb@### avoid p-values == 0@\\
\mbox{}\verb@.pm <- function(x) (1 + sum(x)) / (1 + length(x))@\\
\mbox{}\verb@if (alternative == "two.sided" && length(cf) > 1L) {@\\
\mbox{}\verb@    STATISTIC <- c("Perm chi-squared" = sum(Esc * solve(x$perm$Covariance, Esc)))@\\
\mbox{}\verb@    ps <- x$perm$permStat@\\
\mbox{}\verb@    if (!is.null(x$perm$permStat))@\\
\mbox{}\verb@        PVAL <- .pm(ps > STATISTIC + tol)@\\
\mbox{}\verb@    else {@\\
\mbox{}\verb@        DF <- c("df" = x$perm$DF)@\\
\mbox{}\verb@        PVAL <- pchisq(STATISTIC, df = DF, lower.tail = FALSE)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@} else {@\\
\mbox{}\verb@    STATISTIC <- c("Perm Z" = Esc / sqrt(c(x$perm$Covariance)))@\\
\mbox{}\verb@    if (!is.null(x$perm$permStat)) {@\\
\mbox{}\verb@        if (alternative == "two.sided")@\\
\mbox{}\verb@            PVAL <- .pm(abs(x$perm$permStat) > abs(STATISTIC) + tol)@\\
\mbox{}\verb@        else if (alternative == "less")@\\
\mbox{}\verb@            PVAL <- .pm(x$perm$permStat < STATISTIC - tol)@\\
\mbox{}\verb@        else@\\
\mbox{}\verb@            PVAL <- .pm(x$perm$permStat > STATISTIC + tol)@\\
\mbox{}\verb@    } else {@\\
\mbox{}\verb@        if (alternative == "two.sided")@\\
\mbox{}\verb@            PVAL <- pchisq(STATISTIC^2, df = 1, lower.tail = FALSE)@\\
\mbox{}\verb@        else@\\
\mbox{}\verb@            PVAL <- pnorm(STATISTIC, lower.tail = alternative == "less")@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb27a}{27a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The mean and variance of the linear permutation statistic under the null was
given by \cite{strasserweber1999}:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap37}\raggedright\small
\NWtarget{nuweb30}{} $\langle\,${\itshape Strasser Weber}\nobreak\ {\footnotesize {30}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.SW <- function(res, xt) {@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (length(dim(xt)) == 3L) {@\\
\mbox{}\verb@        res <- matrix(res, nrow = dim(xt)[1L], ncol = dim(xt)[3])@\\
\mbox{}\verb@        STAT <-  Exp <- Cov <- 0@\\
\mbox{}\verb@        for (b in seq_len(dim(xt)[3L])) {@\\
\mbox{}\verb@            sw <- .SW(res[,b, drop = TRUE], xt[,,b, drop = TRUE])@\\
\mbox{}\verb@            STAT <- STAT + sw$Statistic@\\
\mbox{}\verb@            Exp <- Exp + sw$Expectation@\\
\mbox{}\verb@            Cov <- Cov + sw$Covariance@\\
\mbox{}\verb@        }@\\
\mbox{}\verb@        return(list(Statistic = STAT, Expectation = as.vector(Exp),@\\
\mbox{}\verb@                    Covariance = Cov))@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@@\\
\mbox{}\verb@    Y <- matrix(res, ncol = 1, nrow = length(xt))@\\
\mbox{}\verb@    weights <- c(xt)@\\
\mbox{}\verb@    x <- gl(ncol(xt), nrow(xt))@\\
\mbox{}\verb@    X <- model.matrix(~ x, data = data.frame(x = x))[,-1L,drop = FALSE]@\\
\mbox{}\verb@@\\
\mbox{}\verb@    w. <- sum(weights)@\\
\mbox{}\verb@    wX <- weights * X@\\
\mbox{}\verb@    wY <- weights * Y@\\
\mbox{}\verb@    ExpX <- colSums(wX)@\\
\mbox{}\verb@    ExpY <- colSums(wY) / w.@\\
\mbox{}\verb@    CovX <- crossprod(X, wX)@\\
\mbox{}\verb@    Yc <- t(t(Y) - ExpY)@\\
\mbox{}\verb@    CovY <- crossprod(Yc, weights * Yc) / w.@\\
\mbox{}\verb@    Exp <- kronecker(ExpY, ExpX)@\\
\mbox{}\verb@    Cov <- w. / (w. - 1) * kronecker(CovY, CovX) -@\\
\mbox{}\verb@           1 / (w. - 1) * kronecker(CovY, tcrossprod(ExpX))@\\
\mbox{}\verb@    STAT <- crossprod(X, wY)@\\
\mbox{}\verb@    list(Statistic = STAT, Expectation = as.vector(Exp),@\\
\mbox{}\verb@         Covariance = Cov)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb34}{34}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
For small samples, we used the \code{r2dtable} function to sample from
tables with fixed marginal distributions:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap38}\raggedright\small
\NWtarget{nuweb31}{} $\langle\,${\itshape resampling}\nobreak\ {\footnotesize {31}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.resample <- function(res, xt, B = 10000) {@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (length(dim(xt)) == 2L)@\\
\mbox{}\verb@        xt <- as.table(array(xt, dim = c(dim(xt), 1)))@\\
\mbox{}\verb@@\\
\mbox{}\verb@    res <- matrix(res, nrow = dim(xt)[1L], ncol = dim(xt)[3L])@\\
\mbox{}\verb@    stat <- 0@\\
\mbox{}\verb@    ret <- .SW(res, xt)@\\
\mbox{}\verb@    if (dim(xt)[2L] == 2L) {@\\
\mbox{}\verb@        ret$testStat <- c((ret$Statistic - ret$Expectation) / sqrt(c(ret$Covariance)))@\\
\mbox{}\verb@    } else {@\\
\mbox{}\verb@        ES <- ret$Statistic - ret$Expectation@\\
\mbox{}\verb@        ret$testStat <- sum(ES * solve(ret$Covariance, ES))@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    ret$DF <- dim(xt)[2L] - 1L@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (B) {@\\
\mbox{}\verb@        for (j in 1:dim(xt)[3L]) {@\\
\mbox{}\verb@           rt <- r2dtable(B, r = rowSums(xt[,,j]), c = colSums(xt[,,j]))@\\
\mbox{}\verb@           stat <- stat + sapply(rt, function(x) colSums(x[,-1L, drop = FALSE] * res[,j]))@\\
\mbox{}\verb@        }@\\
\mbox{}\verb@        if (dim(xt)[2L] == 2L) {@\\
\mbox{}\verb@             ret$permStat <- (stat - ret$Expectation) / sqrt(c(ret$Covariance))@\\
\mbox{}\verb@        } else {@\\
\mbox{}\verb@            ES <- matrix(stat, ncol = B) - ret$Expectation@\\
\mbox{}\verb@            ret$permStat <- rowSums(crossprod(ES, solve(ret$Covariance, ES)))@\\
\mbox{}\verb@        }@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    ret@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb34}{34}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
As an example, consider the Wilcoxon rank sum test, where the scores under
the null are a linear function of the ranks of the data. We compute the
asymptotic and approximated reference distribution and corresponding
p-values for a test statistics in quadratic form:

<<SW>>=
set.seed(29)
w <- gl(2, 15)
(s <- .SW(r <- rank(u <- runif(length(w))), model.matrix(~ 0 + w)))
ps <- .resample(r, model.matrix(~ 0 + w), B = 100000)
ps$testStat^2
mean(abs(ps$permStat) > abs(ps$testStat) - .Machine$double.eps)
pchisq(ps$testStat^ifelse(ps$DF == 1, 2, 1), df = ps$DF, lower.tail = FALSE)
### exactly the same
kruskal.test(u ~ w)
library("coin")
### almost the same
kruskal_test(u ~ w, distribution = approximate(100000))
@

<TH>Ordered alternatives: Use contrast based tests in multcomp</TH>

\chapter{Distribution-free Tests in Stratified $K$-sample Oneway Layouts}

\section{\code{free1way}}

We provide a new test procedure in a generic \code{free1way}, featuring
a method for tables (the main workhorse) and additional user interfaces. 

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap39}\raggedright\small
\NWtarget{nuweb33}{} $\langle\,${\itshape link2fun}\nobreak\ {\footnotesize {33}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@if (!inherits(link, "linkfun")) {@\\
\mbox{}\verb@    link <- match.arg(link)@\\
\mbox{}\verb@    link <- do.call(link, list())@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb34}{34}\NWlink{nuweb65}{, 65}\NWlink{nuweb66}{, 66}\NWlink{nuweb68a}{, 68a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
<TH>\code{B = 0} comes from \code{chisq.test} and means the default
asymptotic permutation distribution. \code{B = 1000} means 1000 random
permuations. Can we use \code{B = Inf} for the exact distribution once
available?</TH>

We use the positive residuals for defining a permutation test with treatment
effect coding using the first group as control, that is, the test statistic
is defined through the sum of the positive residuals in all but the control
group. Unfortunately, most \code{stats::*.test} procedures use the second
group as control, so factors need to be releveled to obtain identical
results (this is relevant for the one-sided case).

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap40}\raggedright\small
\NWtarget{nuweb34}{} $\langle\,${\itshape free1way}\nobreak\ {\footnotesize {34}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@free1way <- function(y, ...)@\\
\mbox{}\verb@    UseMethod("free1way")@\\
\mbox{}\verb@@\\
\mbox{}\verb@free1way.table <- function(y, link = c("logit", "probit", "cloglog", "loglog"), @\\
\mbox{}\verb@                           mu = 0, B = 0, ...)@\\
\mbox{}\verb@{@\\
\mbox{}\verb@@\\
\mbox{}\verb@    cl <- match.call()@\\
\mbox{}\verb@@\\
\mbox{}\verb@    d <- dim(y)@\\
\mbox{}\verb@    dn <- dimnames(y)@\\
\mbox{}\verb@    DNAME <- NULL@\\
\mbox{}\verb@    if (!is.null(dn)) {@\\
\mbox{}\verb@        DNAME <- paste(names(dn)[1], "by", names(dn)[2], @\\
\mbox{}\verb@                       paste0("(", paste0(dn[2], collapse = ", "), ")"))@\\
\mbox{}\verb@        if (length(dn) == 3L)@\\
\mbox{}\verb@            DNAME <- paste(DNAME, "\n\t stratified by", names(dn)[3])@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape link2fun}\nobreak\ {\footnotesize \NWlink{nuweb33}{33}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ret <- .free1wayML(y, link = link, mu = mu, ...)@\\
\mbox{}\verb@    ret$link <- link@\\
\mbox{}\verb@    ret$data.name <- DNAME@\\
\mbox{}\verb@    ret$call <- cl@\\
\mbox{}\verb@@\\
\mbox{}\verb@    alias <- link$alias@\\
\mbox{}\verb@    if (length(link$alias) == 2L) alias <- alias[1L + (d[2] > 2L)]@\\
\mbox{}\verb@    stratified <- FALSE@\\
\mbox{}\verb@    if (length(d) == 3L) stratified <- d[3L] > 1@\\
\mbox{}\verb@    ret$method <- paste(ifelse(stratified, "Stratified", ""), @\\
\mbox{}\verb@                        paste0(d[2L], "-sample"), alias, @\\
\mbox{}\verb@                        "test against", link$model, "alternatives")@\\
\mbox{}\verb@@\\
\mbox{}\verb@    cf <- ret$par@\\
\mbox{}\verb@    cf[idx <- seq_len(d[2L] - 1L)] <- 0@\\
\mbox{}\verb@    pr <- ret$profile(cf, idx)@\\
\mbox{}\verb@    res <- - pr$negresiduals@\\
\mbox{}\verb@    if (d[2L] == 2L)@\\
\mbox{}\verb@        res <- res / sqrt(c(pr$hessian))@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape Strasser Weber}\nobreak\ {\footnotesize \NWlink{nuweb30}{30}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape resampling}\nobreak\ {\footnotesize \NWlink{nuweb31}{31}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (length(dim(y)) == 3L) y <- y[,,ret$strata, drop = FALSE]@\\
\mbox{}\verb@    if (length(dim(y)) == 4L) {@\\
\mbox{}\verb@        y <- y[,,ret$strata,, drop = FALSE]@\\
\mbox{}\verb@        dy <- dim(y)@\\
\mbox{}\verb@        dy[1] <- dy[1] * 2@\\
\mbox{}\verb@        y <- apply(y, 3, function(x) rbind(x[,,2], x[,,1]))@\\
\mbox{}\verb@        y <- array(y, dim = dy[1:3])@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    ret$perm <- .resample(res, y, B = B)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (!is.null(names(dn))) {@\\
\mbox{}\verb@        fm <- as.formula(paste(names(dn)[1:2], collapse = "~"))@\\
\mbox{}\verb@        ret$terms <- terms(fm, data = as.data.frame(y))@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@@\\
\mbox{}\verb@    class(ret) <- "free1way"@\\
\mbox{}\verb@    return(ret)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb20a}{20a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The \code{formula} method allows formulae \code{outcome ~ treatment |
stratum)} for model specification

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap41}\raggedright\small
\NWtarget{nuweb36}{} $\langle\,${\itshape free1way formula}\nobreak\ {\footnotesize {36}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@free1way.formula <- function(formula, data, weights, subset, na.action = na.pass, ...)@\\
\mbox{}\verb@{@\\
\mbox{}\verb@    cl <- match.call()@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if(missing(formula) || (length(formula) != 3L))@\\
\mbox{}\verb@        stop("'formula' missing or incorrect")@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (stratum <- (length(formula[[3L]]) > 1)) {@\\
\mbox{}\verb@      if ((length(formula[[3L]]) != 3L) || @\\
\mbox{}\verb@          (formula[[3L]][[1L]] != as.name("|")) || @\\
\mbox{}\verb@          (length(formula[[3L]][[2L]]) !=  1L) || @\\
\mbox{}\verb@          (length(formula[[3L]][[3L]]) != 1L)) @\\
\mbox{}\verb@        stop("incorrect specification for 'formula'")@\\
\mbox{}\verb@      formula[[3L]][[1L]] <- as.name("+")@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@@\\
\mbox{}\verb@    formula <- terms(formula)@\\
\mbox{}\verb@    if (length(attr(formula, "term.labels")) > 1L + stratum)@\\
\mbox{}\verb@        stop("'formula' missing or incorrect")@\\
\mbox{}\verb@    group <- attr(formula, "term.labels")[1L]@\\
\mbox{}\verb@@\\
\mbox{}\verb@    m <- match.call(expand.dots = FALSE)@\\
\mbox{}\verb@    m$formula <- formula@\\
\mbox{}\verb@    if (is.matrix(eval(m$data, parent.frame())))@\\
\mbox{}\verb@        m$data <- as.data.frame(data)@\\
\mbox{}\verb@    ## need stats:: for non-standard evaluation@\\
\mbox{}\verb@    m[[1L]] <- quote(stats::model.frame)@\\
\mbox{}\verb@    m$... <- NULL@\\
\mbox{}\verb@    mf <- eval(m, parent.frame())@\\
\mbox{}\verb@    response <- attr(attr(mf, "terms"), "response")@\\
\mbox{}\verb@    DNAME <- paste(vn <- c(names(mf)[response], group), collapse = " by ") # works in all cases@\\
\mbox{}\verb@    w <- as.vector(model.weights(mf))@\\
\mbox{}\verb@    y <- mf[[response]]@\\
\mbox{}\verb@    event <- NULL@\\
\mbox{}\verb@    if (inherits(y, "Surv")) {@\\
\mbox{}\verb@        if (attr(y, "type") != "right")@\\
\mbox{}\verb@            stop("free1way only supports right-censoring")@\\
\mbox{}\verb@        event <- (y[,2] > 0)@\\
\mbox{}\verb@        y <- y[,1]@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    g <- mf[[group]]@\\
\mbox{}\verb@    stopifnot(is.factor(g))@\\
\mbox{}\verb@    lev <- levels(g)@\\
\mbox{}\verb@    DNAME <- paste(DNAME, paste0("(", paste0(lev, collapse = ", "), ")"))@\\
\mbox{}\verb@    if (nlevels(g) < 2L)@\\
\mbox{}\verb@        stop("grouping factor must have at least 2 levels")@\\
\mbox{}\verb@    if (stratum) {@\\
\mbox{}\verb@        st <- factor(mf[[3L]], levels = )@\\
\mbox{}\verb@        if (nlevels(st) < 2L)@\\
\mbox{}\verb@            stop("at least two strata must be present")@\\
\mbox{}\verb@        vn <- c(vn, names(mf)[3L])@\\
\mbox{}\verb@        RVAL <- free1way(y = y, groups = g, blocks = st, event = event, weights = w,@\\
\mbox{}\verb@                         varnames = vn, ...)@\\
\mbox{}\verb@        DNAME <- paste(DNAME, paste("\n\t stratified by", names(mf)[3L]))@\\
\mbox{}\verb@    } else {@\\
\mbox{}\verb@        ## Call the corresponding method@\\
\mbox{}\verb@        RVAL <- free1way(y = y, groups = g, event = event, weights = w, varnames = vn, ...)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    RVAL$data.name <- DNAME@\\
\mbox{}\verb@    RVAL$call <- cl@\\
\mbox{}\verb@    RVAL@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb20a}{20a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The method for numeric outcomes provides a discretisation at the unique
observed outcome values, or (for very large sample sizes), for binned
outcomes. The \code{event} argument is a logical where \code{TRUE} is
interpreted as an event and \code{FALSE} as right-censored observation

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap42}\raggedright\small
\NWtarget{nuweb37a}{} $\langle\,${\itshape variable names and checks}\nobreak\ {\footnotesize {37a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@cl <- match.call()@\\
\mbox{}\verb@stopifnot(is.factor(groups))@\\
\mbox{}\verb@stopifnot(nlevels(groups) > 1L)@\\
\mbox{}\verb@DNAME <- paste(varnames[1], "by", varnames[2])@\\
\mbox{}\verb@DNAME <- paste(DNAME, paste0("(", paste0(levels(groups), collapse = ", "), ")"))@\\
\mbox{}\verb@@\\
\mbox{}\verb@if (!is.null(blocks)) {@\\
\mbox{}\verb@    stopifnot(is.factor(blocks))@\\
\mbox{}\verb@    stopifnot(nlevels(blocks) > 1L)@\\
\mbox{}\verb@    DNAME <- paste(DNAME, "\n\t stratified by", varnames[3])@\\
\mbox{}\verb@}@\\
\mbox{}\verb@varnames <- varnames[varnames != "NULL"]@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb37b}{37b}\NWlink{nuweb38}{, 38}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap43}\raggedright\small
\NWtarget{nuweb37b}{} $\langle\,${\itshape free1way numeric}\nobreak\ {\footnotesize {37b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@free1way.numeric <- function(y, groups, blocks = NULL, event = NULL, weights = NULL, nbins = 0, @\\
\mbox{}\verb@    varnames = c(deparse1(substitute(y)), @\\
\mbox{}\verb@                 deparse1(substitute(groups)), @\\
\mbox{}\verb@                 deparse1(substitute(blocks))), ...) {@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape variable names and checks}\nobreak\ {\footnotesize \NWlink{nuweb37a}{37a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (!is.null(event)) {@\\
\mbox{}\verb@        stopifnot(is.logical(event))@\\
\mbox{}\verb@        uy <- sort(unique(y[event]))@\\
\mbox{}\verb@        if (all(y[!event] < uy[length(uy)]))@\\
\mbox{}\verb@            uy <- uy[-length(uy)]@\\
\mbox{}\verb@    } else {@\\
\mbox{}\verb@        uy <- sort(unique(y))@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    if (nbins && nbins < length(uy)) {@\\
\mbox{}\verb@        nbins <- ceiling(nbins)@\\
\mbox{}\verb@        breaks <- c(-Inf, quantile(y, probs = seq_len(nbins) / (nbins + 1L)), Inf)@\\
\mbox{}\verb@    } else {@\\
\mbox{}\verb@        breaks <- c(-Inf, uy, Inf)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    r <- cut(y, breaks = breaks, ordered_result = TRUE)[, drop = TRUE]@\\
\mbox{}\verb@    RVAL <- free1way(y = r, groups = groups, blocks = blocks, @\\
\mbox{}\verb@                     event = event, weights = weights, @\\
\mbox{}\verb@                     varnames = varnames, ...)@\\
\mbox{}\verb@    RVAL$data.name <- DNAME@\\
\mbox{}\verb@    RVAL$call <- cl@\\
\mbox{}\verb@    RVAL@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb20a}{20a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The \code{factor} method also allows right-censoring but otherwise is just a
call to \code{xtabs}:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap44}\raggedright\small
\NWtarget{nuweb38}{} $\langle\,${\itshape free1way factor}\nobreak\ {\footnotesize {38}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@free1way.factor <- function(y, groups, blocks = NULL, event = NULL, weights = NULL, @\\
\mbox{}\verb@    varnames = c(deparse1(substitute(y)), @\\
\mbox{}\verb@                 deparse1(substitute(groups)), @\\
\mbox{}\verb@                 deparse1(substitute(blocks))), ...) {@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape variable names and checks}\nobreak\ {\footnotesize \NWlink{nuweb37a}{37a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (nlevels(y) > 2L)@\\
\mbox{}\verb@        stopifnot(is.ordered(y))@\\
\mbox{}\verb@    d <- data.frame(w = 1, y = y, groups = groups)@\\
\mbox{}\verb@    if (!is.null(weights)) d$w <- weights@\\
\mbox{}\verb@    if (is.null(blocks)) blocks <- gl(1, nrow(d))@\\
\mbox{}\verb@    d$blocks <- blocks @\\
\mbox{}\verb@    if (!is.null(event)) {@\\
\mbox{}\verb@        stopifnot(is.logical(event))@\\
\mbox{}\verb@        d$event <- event@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    tab <- xtabs(w ~ ., data = d)@\\
\mbox{}\verb@    dn <- dimnames(tab)@\\
\mbox{}\verb@    names(dn)[seq_along(varnames)] <- varnames@\\
\mbox{}\verb@    dimnames(tab) <- dn@\\
\mbox{}\verb@    RVAL <- free1way(tab, ...)@\\
\mbox{}\verb@    RVAL$data.name <- DNAME@\\
\mbox{}\verb@    RVAL$call <- cl@\\
\mbox{}\verb@    RVAL@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb20a}{20a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\section{\code{free1way} Methods}

We start with \code{coef}, \code{vcov}, and
\code{model.frame}/\code{model.matrix} methods such that multiple comparison
procedures from \pkg{multcomp} will work out of the box. The \code{coef}
method allows to obtain effects at alternative scales: probabilistic indices
(\code{AUC} = \code{PI}) or the overlap coefficient:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap45}\raggedright\small
\NWtarget{nuweb39}{} $\langle\,${\itshape free1way methods}\nobreak\ {\footnotesize {39}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@coef.free1way <- function(object, what = c("shift", "PI", "AUC", "OVL"), ...)@\\
\mbox{}\verb@{@\\
\mbox{}\verb@    what <- match.arg(what)@\\
\mbox{}\verb@    cf <- object$coefficients@\\
\mbox{}\verb@    return(switch(what, "shift" = cf,@\\
\mbox{}\verb@                        "PI" = object$link$parm2PI(cf),@\\
\mbox{}\verb@                        "AUC" = object$link$parm2PI(cf),        ### same as PI@\\
\mbox{}\verb@                        "OVL" = object$link$parm2OVL(cf)))@\\
\mbox{}\verb@}@\\
\mbox{}\verb@vcov.free1way <- function(object, ...)@\\
\mbox{}\verb@    object$vcov@\\
\mbox{}\verb@logLik.free1way <- function(object, ...)@\\
\mbox{}\verb@    -object$value@\\
\mbox{}\verb@### the next two could go into multcomp@\\
\mbox{}\verb@model.frame.free1way <- function(formula, ...)@\\
\mbox{}\verb@    as.data.frame(formula$table)@\\
\mbox{}\verb@model.matrix.free1way <- function (object, ...) @\\
\mbox{}\verb@{@\\
\mbox{}\verb@    mm <- model.matrix(delete.response(terms(object)), data = model.frame(object))@\\
\mbox{}\verb@    at <- attributes(mm)@\\
\mbox{}\verb@    mm <- mm[, -1]@\\
\mbox{}\verb@    at$dim[2] <- at$dim[2] - 1@\\
\mbox{}\verb@    at$dimnames[[2]] <- at$dimnames[[2]][-1]@\\
\mbox{}\verb@    at$assign <- at$assign[-1]@\\
\mbox{}\verb@    attributes(mm) <- at@\\
\mbox{}\verb@    mm@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb20a}{20a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
We use the \code{print} method to report different test statistics and
corresponding $p$-values via the \code{test} and \code{alternative}
arguments. The reason for doing so is that the parameter estimation only
needs to be performed once in cases users are interested in different
tests or (see below) confidence intervals. By default, an asymptotic  
permutation test is performed, mainly because the $p$-values coincide with
some special cases (\code{wilcox,kruskal,friedman.test}):

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap46}\raggedright\small
\NWtarget{nuweb40}{} $\langle\,${\itshape free1way print}\nobreak\ {\footnotesize {40}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.print.free1way <- function(x, test = c("Permutation", "Wald", "LRT", "Rao"), @\\
\mbox{}\verb@                            alternative = c("two.sided", "less", "greater"), @\\
\mbox{}\verb@                            tol = sqrt(.Machine$double.eps), ...)@\\
\mbox{}\verb@{@\\
\mbox{}\verb@@\\
\mbox{}\verb@    test <- match.arg(test)@\\
\mbox{}\verb@    alternative <- match.arg(alternative)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ### global@\\
\mbox{}\verb@    cf <- coef(x)@\\
\mbox{}\verb@    if ((length(cf) > 1L || test == "LRT") && alternative != "two.sided") @\\
\mbox{}\verb@        stop("Cannot compute one-sided p-values")@\\
\mbox{}\verb@@\\
\mbox{}\verb@    DF <- NULL@\\
\mbox{}\verb@    parm <- seq_along(cf)@\\
\mbox{}\verb@    value <- 0@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape statistics}\nobreak\ {\footnotesize \NWlink{nuweb27a}{27a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    RVAL <- list(statistic = STATISTIC, parameter = DF, p.value = PVAL, @\\
\mbox{}\verb@        null.value = x$mu, alternative = alternative, method = x$method, @\\
\mbox{}\verb@        data.name = x$data.name)@\\
\mbox{}\verb@    class(RVAL) <- "htest"@\\
\mbox{}\verb@    return(RVAL)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@\\
\mbox{}\verb@print.free1way <- function(x, ...) {@\\
\mbox{}\verb@    print(ret <- .print.free1way(x, ...))@\\
\mbox{}\verb@    return(invisible(x))@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb20a}{20a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The \code{summary} method performs population Wald inference unless the
\code{test} argument is specified:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap47}\raggedright\small
\NWtarget{nuweb41}{} $\langle\,${\itshape free1way summary}\nobreak\ {\footnotesize {41}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@summary.free1way <- function(object, test, alternative = c("two.sided", "less", "greater"), @\\
\mbox{}\verb@                             tol = .Machine$double.eps, ...)@\\
\mbox{}\verb@{@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (!missing(test))@\\
\mbox{}\verb@        return(.print.free1way(object, test = test, alternative = alternative, tol = tol))@\\
\mbox{}\verb@   @\\
\mbox{}\verb@    alternative <- match.arg(alternative)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ESTIMATE <- coef(object)@\\
\mbox{}\verb@    SE <- sqrt(diag(vcov(object)))@\\
\mbox{}\verb@    STATISTIC <- unname(ESTIMATE / SE)@\\
\mbox{}\verb@    if (alternative == "less") {@\\
\mbox{}\verb@        PVAL <- pnorm(STATISTIC)@\\
\mbox{}\verb@    } else if (alternative == "greater") {@\\
\mbox{}\verb@        PVAL <- pnorm(STATISTIC, lower.tail = FALSE)@\\
\mbox{}\verb@    } else {@\\
\mbox{}\verb@        PVAL <- 2 * pnorm(-abs(STATISTIC))@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    cfmat <- cbind(ESTIMATE, SE, STATISTIC, PVAL)@\\
\mbox{}\verb@    colnames(cfmat) <- c(object$link$parm, "Std. Error", "z value",@\\
\mbox{}\verb@                         switch(alternative, "two.sided" = "P(>|z|)",@\\
\mbox{}\verb@                                             "less" = "P(<z)",@\\
\mbox{}\verb@                                             "greater" = "P(>z)"))@\\
\mbox{}\verb@    ret <- list(call = object$call, coefficients = cfmat)@\\
\mbox{}\verb@    class(ret) <- "summary.free1way"@\\
\mbox{}\verb@    return(ret)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@print.summary.free1way <- function(x, ...) {@\\
\mbox{}\verb@    cat("\nCall:\n", paste(deparse(x$call), sep = "\n", collapse = "\n"), @\\
\mbox{}\verb@        "\n\n", sep = "")@\\
\mbox{}\verb@    cat("Coefficients:\n")@\\
\mbox{}\verb@    printCoefmat(x$coefficients)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb20a}{20a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
Confidence intervals are computed by inversion of the corresponding test
statistics. Because LRT and Rao confidence intervals are invariant wrt to
transformations, proper LRT or Rao confidence intervals for probabilistic
indices or overlap coefficients can also be computed. The computation always
starts with Wald intervals, which are either returned or used as starting
values for the inversion:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap48}\raggedright\small
\NWtarget{nuweb42}{} $\langle\,${\itshape free1way confint}\nobreak\ {\footnotesize {42}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@confint.free1way <- function(object, parm,@\\
\mbox{}\verb@    level = .95, test = c("Permutation", "Wald", "LRT", "Rao"), @\\
\mbox{}\verb@    what = c("shift", "PI", "AUC", "OVL"), ...)@\\
\mbox{}\verb@{@\\
\mbox{}\verb@@\\
\mbox{}\verb@    test <- match.arg(test)@\\
\mbox{}\verb@    conf.level <- 1 - (1 - level) / 2@\\
\mbox{}\verb@@\\
\mbox{}\verb@    cf <- coef(object)@\\
\mbox{}\verb@    if (missing(parm)) @\\
\mbox{}\verb@        parm <- seq_along(cf)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    CINT <- confint.default(object, level = level)@\\
\mbox{}\verb@    if (test == "Wald")@\\
\mbox{}\verb@        return(CINT)@\\
\mbox{}\verb@    wlevel <- level@\\
\mbox{}\verb@    wlevel <- 1 - (1 - level) / 10@\\
\mbox{}\verb@    CINT[] <- confint.default(object, level = wlevel)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    sfun <- function(value, parm, quantile) {@\\
\mbox{}\verb@        x <- object@\\
\mbox{}\verb@        alternative <- "two.sided"@\\
\mbox{}\verb@        tol <- .Machine$double.eps@\\
\mbox{}\verb@        @\hbox{$\langle\,${\itshape statistics}\nobreak\ {\footnotesize \NWlink{nuweb27a}{27a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@        return(STATISTIC - quantile)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (test == "Permutation") {@\\
\mbox{}\verb@        stopifnot(length(cf) == 1L)@\\
\mbox{}\verb@        if (is.null(object$perm$permStat)) {@\\
\mbox{}\verb@            qu <- qnorm(conf.level) * c(-1, 1)@\\
\mbox{}\verb@        } else {@\\
\mbox{}\verb@            qu <- quantile(object$perm$permStat, @\\
\mbox{}\verb@                           probs = c(1 - conf.level, conf.level))@\\
\mbox{}\verb@            att.level <- mean(object$perm$permStat > qu[1] & @\\
\mbox{}\verb@                              object$perm$permStat < qu[2])@\\
\mbox{}\verb@            attr(CINT, "Attained level") <- att.level@\\
\mbox{}\verb@        }@\\
\mbox{}\verb@    } else {@\\
\mbox{}\verb@        qu <- rep.int(qchisq(level, df = 1), 2) ### always two.sided@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@@\\
\mbox{}\verb@    for (p in parm) {@\\
\mbox{}\verb@        CINT[p, 1] <- uniroot(sfun, interval = c(CINT[p,1], cf[p]), @\\
\mbox{}\verb@                              parm = p, quantile = qu[2])$root@\\
\mbox{}\verb@        CINT[p, 2] <- uniroot(sfun, interval = c(cf[p], CINT[p, 2]), @\\
\mbox{}\verb@                              parm = p, quantile = qu[1])$root@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@@\\
\mbox{}\verb@    what <- match.arg(what)@\\
\mbox{}\verb@    CINT <- switch(what, "shift" = CINT,@\\
\mbox{}\verb@                         "PI" = object$link$parm2PI(CINT),@\\
\mbox{}\verb@                         "AUC" = object$link$parm2PI(CINT), ### same as PI @\\
\mbox{}\verb@                         "OVL" = object$link$parm2OVL(CINT))@\\
\mbox{}\verb@    return(CINT)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb20a}{20a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
As an example, we compute log-odds ratios for the table introduced above and
report some tests and confidence intervals:

<<free>>=
x
### asymptotic permutation test
(ft <- free1way(x))
coef(ft)
vcov(ft)
### Wald per parameter
summary(ft)
library("multcomp")
summary(glht(ft), test = univariate())

### global Wald
summary(ft, test = "Wald")
summary(glht(ft), test = Chisqtest())

### Rao score, Permutation score, LRT
summary(ft, test = "Rao")
summary(ft, test = "Permutation")
summary(ft, test = "LRT")

### Wald confidence intervals, unadjusted
confint(glht(ft), calpha = univariate_calpha())
confint(ft, test = "Wald")

### Rao and LRT intervals
confint(ft, test = "Rao")
confint(ft, test = "LRT")
@

\section{Wilcoxon Test}

The second example is a Wilcoxon test for a single log-odds ratio
comparing to treatment groups:

<<formula>>=
N <- 25
w <- gl(2, N)
y <- rlogis(length(w), location = c(0, 1)[w])

#### link = logit is default
ft <- free1way(y ~ w)

### Wald 
summary(ft)

### Permutation test
wilcox.test(y ~ w, alternative = "greater", correct = FALSE)$p.value
pvalue(wilcox_test(y ~ w, alternative = "greater"))
summary(ft, test = "Permutation", alternative = "less")$p.value
wilcox.test(y ~ w, alternative = "less", correct = FALSE)$p.value
pvalue(wilcox_test(y ~ w, alternative = "less"))
summary(ft, test = "Permutation", alternative = "greater")$p.value
wilcox.test(y ~ w, correct = FALSE)$p.value
kruskal.test(y ~ w)$p.value
pvalue(wilcox_test(y ~ w))
summary(ft, test = "Permutation")$p.value

### Wald tests
summary(ft, test = "Wald", alternative = "less")
summary(ft, test = "Wald", alternative = "greater")
summary(ft, test = "Wald")

### Rao score tests
summary(ft, test = "Rao", alternative = "less")
summary(ft, test = "Rao", alternative = "greater")
summary(ft, test = "Rao")

### LRT (only two-sided)
summary(ft, test = "LRT")

### confidence intervals for log-odds ratios
confint(ft, test = "Permutation")
confint(ft, test = "LRT")
confint(ft, test = "Wald")
confint(ft, test = "Rao")

### confidence interval for "Wilcoxon Parameter" = PI = AUC
confint(ft, test = "Rao", what = "AUC")

### comparison with rms::orm
library("rms")
rev(coef(or <- orm(y ~ w)))[1]
coef(ft)
logLik(or)
logLik(ft)
vcov(or)[2,2]
vcov(ft)
ci <- confint(or)
ci[nrow(ci),]
confint(ft, test = "Wald")
@


\section{Mantel-Haenszel Test}

<<mh>>=
example(mantelhaen.test, echo = FALSE)
mantelhaen.test(UCBAdmissions, correct = FALSE)
ft <- free1way(UCBAdmissions)
summary(ft, test = "Wald")
exp(coef(ft))
exp(confint(ft, test = "Wald"))
exp(sapply(dimnames(UCBAdmissions)[[3L]], function(dept)
       confint(free1way(UCBAdmissions[,,dept]), test = "Permutation")))
sapply(dimnames(UCBAdmissions)[[3L]], function(dept)
       fisher.test(UCBAdmissions[,,dept], conf.int = TRUE)$conf.int)
@

\section{\code{prop.test}}

<<pt>>=
prop.test(UCBAdmissions[,,1], correct = FALSE)
summary(free1way(UCBAdmissions[,,1]), test = "Rao")
@


\section{Kruskal-Wallis Test}

<<kw>>=
example(kruskal.test, echo = FALSE)
kruskal.test(x ~ g)
free1way(x ~ g)
@

\section{Savage Test}

We start without censoring (Savage test) and add strata

<<sw>>=
library("survival")
N <- 10
nd <- expand.grid(g = gl(3, N), s = gl(3, N))
nd$tm <- rexp(nrow(nd))
nd$ev <- TRUE
survdiff(Surv(tm, ev) ~ g + strata(s), data = nd, rho = 0)$chisq
cm <- coxph(Surv(tm, ev) ~ g + strata(s), data = nd)

(ft <- free1way(tm ~ g | s, data = nd, link = "cloglog"))
coef(cm)
coef(ft)
vcov(cm)
vcov(ft)
summary(ft)
summary(cm)$sctest
summary(ft, test = "Rao")
summary(cm)$logtest
summary(ft, test = "LRT")
summary(cm)$waldtest
summary(ft, test = "Wald")
summary(ft, test = "Permutation")

library("coin")
independence_test(Surv(tm, ev) ~ g | s, data = nd, ytrafo = function(...)
                  trafo(..., numeric_trafo = logrank_trafo, block = nd$s), teststat = "quad")
@

Wilcoxon against proportional odds

<<Peto>>=
survdiff(Surv(tm, ev) ~ g + strata(s), data = nd, rho = 1)$chisq
(ft <- free1way(tm ~ g | s, data = nd, link = "logit"))
summary(ft)
summary(ft, test = "Rao")
summary(ft, test = "LRT")
summary(ft, test = "Wald")
summary(ft, test = "Permutation")
@

\section{Log-rank Test}

And now with censoring. We cannot expect this to be identical with what
\pkg{survival} reports, as this package is based on the partial likelihood
and we operate on the full likelihood.

<<sw>>=
library("survival")
data("GBSG2", package = "TH.data")
survdiff(Surv(time, cens) ~ horTh + strata(tgrade), data = GBSG2, rho = 0)$chisq
cm <- coxph(Surv(time, cens) ~ horTh + strata(tgrade), data = GBSG2)

### no formula interface yet
ft <- with(GBSG2, free1way(Surv(time, cens) ~ horTh | tgrade, 
                           link = "cloglog"))
coef(cm)
coef(ft)
vcov(cm)
vcov(ft)
summary(ft)
summary(cm)$sctest
summary(ft, test = "Rao")
summary(cm)$logtest
summary(ft, test = "LRT")
summary(cm)$waldtest
summary(ft, test = "Wald")
summary(ft, test = "Permutation")

### test with many small strata 
(ft <- with(GBSG2, free1way(Surv(time, cens) ~ horTh | pnodes, 
                            link = "cloglog")))
@

Wilcoxon against proportional odds

<<Peto>>=
survdiff(Surv(time, cens) ~ horTh + strata(tgrade), data = GBSG2, rho = 1)$chisq
(ft <- with(GBSG2, free1way(Surv(time, cens) ~ horTh | tgrade, 
                            link = "logit")))
summary(ft)
summary(ft, test = "Rao")
summary(ft, test = "LRT")
summary(ft, test = "Wald")
summary(ft, test = "Permutation")
@


\section{van der Waerden Test}

Normal scores test against a generalised Cohen's $d$:

<<normal>>=
nd$y <- rnorm(nrow(nd))
free1way(y ~ g | s, data = nd, link = "probit")
independence_test(y ~ g | s, data = nd, ytrafo = function(...)
                  trafo(..., numeric_trafo = normal_trafo, block = nd$s), teststat = "quad")
@

\section{Friedman Test}

Each observation is a block

<<friedman>>=
example(friedman.test, echo = FALSE)
rt <- expand.grid(str = gl(22, 1),
                  trt = gl(3, 1, labels = c("Round Out", "Narrow Angle", "Wide Angle")))
rt$tm <- c(RoundingTimes)
friedman.test(RoundingTimes)
(ft <- free1way(tm ~ trt | str, data = rt))
summary(ft)
@

\chapter{Model Diagnostics}

The classical shift model $F_Y(y \mid T = 2) = F_Y(y - \mu \mid T = 1)$
can be critisised using confidence bands for QQ-plots in \code{qqplot},
because the parameter $\mu$ shows up as a vertical shift of the diagonal
if the model is appropriate.

Likewise, model~(\ref{model}) can be graphically assessed using the PP-plot.
We concentrate on the two-sample case. The shift parameter $\delta_2$ gives
rise to the model-based PP graph $(p, F(F^{-1}(p) - \delta_2))$ and a
confidence \emph{band} can be obtained from a confidence \emph{interval} for
$\delta_2$. The PP-plot is, up to rescalings, identical to the ROC curve.

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap49}\raggedright\small
\NWtarget{nuweb57}{} $\langle\,${\itshape ROC bands}\nobreak\ {\footnotesize {57}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@ if (!is.null(conf.level)) {@\\
\mbox{}\verb@    prb <- seq_len(1000) / 1001@\\
\mbox{}\verb@    res <- c(x, y)@\\
\mbox{}\verb@    grp <- gl(2, 1, labels = c(xlab, ylab))@\\
\mbox{}\verb@    grp <- grp[rep(1:2, c(length(x), length(y)))]@\\
\mbox{}\verb@    args <- conf.args@\\
\mbox{}\verb@    args$y <- res@\\
\mbox{}\verb@    args$groups <- grp@\\
\mbox{}\verb@    args$border <- args$col <- args$type <- NULL@\\
\mbox{}\verb@    f1w <- do.call("free1way", args)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ci <- confint(f1w, level = conf.level, type = args$type)@\\
\mbox{}\verb@    lwr <- .p(f1w$link, .q(f1w$link, prb) - ci[1,1])@\\
\mbox{}\verb@    upr <- .p(f1w$link, .q(f1w$link, prb) - ci[1,2])@\\
\mbox{}\verb@    x <- c(prb, rev(prb))@\\
\mbox{}\verb@    y <- c(lwr, rev(upr))@\\
\mbox{}\verb@    xn <- c(x[1L], rep(x[-1L], each = 2))@\\
\mbox{}\verb@    yn <- c(rep(y[-length(y)], each = 2), y[length(y)])@\\
\mbox{}\verb@    polygon(x = xn, y = yn, col = conf.args$col, border = conf.args$border)@\\
\mbox{}\verb@    lines(prb, .p(f1w$link, .q(f1w$link, prb) - coef(f1w)))@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb58}{58}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
We introduce a new function \code{ppplot}, closely following the
implementation of \code{qqplot}, allowing to plot the empirical and
corresponding model-based PP-plot, the latter for a certain choice of link
function:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap50}\raggedright\small
\NWtarget{nuweb58}{} $\langle\,${\itshape ppplot}\nobreak\ {\footnotesize {58}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@ppplot <- function(x, y, plot.it = TRUE,@\\
\mbox{}\verb@                   xlab = deparse1(substitute(x)),@\\
\mbox{}\verb@                   ylab = deparse1(substitute(y)), @\\
\mbox{}\verb@                   ..., conf.level = NULL, @\\
\mbox{}\verb@                   conf.args = list(link = "logit", type = "Wald", @\\
\mbox{}\verb@                                    col = NA, border = NULL)) {@\\
\mbox{}\verb@@\\
\mbox{}\verb@    force(xlab)@\\
\mbox{}\verb@    force(ylab)@\\
\mbox{}\verb@    if (xlab == ylab) {@\\
\mbox{}\verb@        xlab <- paste0("x = ", xlab)@\\
\mbox{}\verb@        ylab <- paste0("y = ", ylab)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ex <- ecdf(x)@\\
\mbox{}\verb@    sy <- sort(unique(y))@\\
\mbox{}\verb@    py <- ecdf(y)(sy)@\\
\mbox{}\verb@    px <- ex(sy)@\\
\mbox{}\verb@    ret <- list(x = px, y = py)@\\
\mbox{}\verb@    if (!plot.it)@\\
\mbox{}\verb@        return(ret)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    plot(px, py, xlim = c(0, 1), ylim = c(0, 1), @\\
\mbox{}\verb@         xlab = xlab, ylab = ylab, type = "n", ...)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape ROC bands}\nobreak\ {\footnotesize \NWlink{nuweb57}{57}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    points(px, py, ...)@\\
\mbox{}\verb@    return(invisible(ret)) @\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb20a}{20a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
Correct logistic model with log-odds ratio three:

\begin{figure}
<<ppplot, fig = TRUE>>=
y <- rlogis(50)
x <- rlogis(50, location = 3)
ppplot(y, x, conf.level = .95)
@
\end{figure}

Incorrect proportional hazards alternative:

\begin{figure}
<<ppplot-savage, fig = TRUE>>=
ppplot(y, x, conf.args = list(link = "cloglog", type = "Wald", 
                              col = NA, border = NULL),
       conf.level = .95)
@
\end{figure}


\chapter{Random Number Generation} \label{ch:rng}

With~\ref{model} we know that
\begin{eqnarray*}
U = F_Y(Y \mid \rS = b, \rT = k) = F\left(F^{-1}\left(F_Y(Y \mid \rS = b, \rT = 1)\right) - \delta_k\right), \quad k = 2, \dots, K
\end{eqnarray*}
follows a standard uniform distribution on the unit interval. This means
that we can sample from the distribution of $Y$ using
\begin{eqnarray*}
F_Y^{-1}\left(F(F^{-1}(U) + \delta_k) \mid \rS = b, \rT = 1)\right).
\end{eqnarray*}
It is therefore enough to draw samples from $F(F^{-1}(U) + \delta_k)$, that
is, assuming a uniform distribution for $F_Y$ in each control group. Because
of the invariance with respect to monotone transformations, transforming all
observations by the same quantile function changes the outcome distributions
but not the shift effects.

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap51}\raggedright\small
\NWtarget{nuweb62}{} $\langle\,${\itshape design args}\nobreak\ {\footnotesize {62}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@K <- length(delta) + 1L@\\
\mbox{}\verb@if (is.null(names(delta))) @\\
\mbox{}\verb@    names(delta) <- LETTERS[seq_len(K)[-1]]@\\
\mbox{}\verb@if (length(alloc_ratio) == 1L) @\\
\mbox{}\verb@    alloc_ratio <- rep_len(alloc_ratio, K - 1)@\\
\mbox{}\verb@stopifnot(length(alloc_ratio) == K - 1)@\\
\mbox{}\verb@if (length(strata_ratio) == 1L) @\\
\mbox{}\verb@    strata_ratio <- rep_len(strata_ratio, B - 1)@\\
\mbox{}\verb@stopifnot(length(strata_ratio) == B - 1)@\\
\mbox{}\verb@### sample size per group (columns) and stratum (rows)@\\
\mbox{}\verb@N <- n * matrix(c(1, alloc_ratio), nrow = B, ncol = K, byrow = TRUE) * @\\
\mbox{}\verb@         matrix(c(1, strata_ratio), nrow = B, ncol = K)@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb65}{65}\NWlink{nuweb68a}{, 68a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap52}\raggedright\small
\NWtarget{nuweb65}{} $\langle\,${\itshape rfree1way}\nobreak\ {\footnotesize {65}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.rfree1way <- function(n, delta = 0, link = c("logit", "probit", "cloglog", "loglog")) {@\\
\mbox{}\verb@@\\
\mbox{}\verb@    logU <- log(ret <- runif(n))@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape link2fun}\nobreak\ {\footnotesize \NWlink{nuweb33}{33}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    trt <- (abs(delta) > 0)@\\
\mbox{}\verb@    ret[trt] <- .p(link, .q(link, logU[trt], log.p = TRUE) + delta[trt])@\\
\mbox{}\verb@@\\
\mbox{}\verb@    return(ret)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@\\
\mbox{}\verb@rfree1way <- function(n, prob = NULL, alloc_ratio = 1, @\\
\mbox{}\verb@                      blocks = ifelse(is.null(prob), 1, NCOL(prob)), @\\
\mbox{}\verb@                      strata_ratio = 1, delta = 0, offset = 0, @\\
\mbox{}\verb@                      link = c("logit", "probit", "cloglog", "loglog"))@\\
\mbox{}\verb@{@\\
\mbox{}\verb@    B <- blocks@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape design args}\nobreak\ {\footnotesize \NWlink{nuweb62}{62}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    rownames(N) <- paste0("block", seq_len(B))@\\
\mbox{}\verb@    ctrl <- "Control"@\\
\mbox{}\verb@    colnames(N) <- c(ctrl, names(delta))@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (length(offset) != K)@\\
\mbox{}\verb@        offset <- rep_len(offset, K)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    trt <- gl(K, 1, labels = colnames(N))@\\
\mbox{}\verb@    blk <- gl(B, 1, labels = rownames(N))@\\
\mbox{}\verb@    ret <- expand.grid(trt = trt, blk = blk)@\\
\mbox{}\verb@    if (B == 1L) ret$blk <- NULL@\\
\mbox{}\verb@    ret <- ret[rep(seq_len(nrow(ret)), times = N), , drop = FALSE]@\\
\mbox{}\verb@    ret$y <- .rfree1way(nrow(ret), @\\
\mbox{}\verb@                        delta = offset[ret$trt] + c(0, delta)[ret$trt], @\\
\mbox{}\verb@                        link = link)@\\
\mbox{}\verb@    if (is.null(prob)) return(ret)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ### return discrete distribution@\\
\mbox{}\verb@    if (!is.matrix(prob))@\\
\mbox{}\verb@        prob <- matrix(prob, nrow = NROW(prob), ncol = B)@\\
\mbox{}\verb@    stopifnot(ncol(prob) == B)@\\
\mbox{}\verb@    prob <- prop.table(prob, margin = 2L)@\\
\mbox{}\verb@    ret <- do.call("rbind", lapply(1:ncol(prob), function(b) {@\\
\mbox{}\verb@        ret <- subset(ret, blk == levels(blk)[b])@\\
\mbox{}\verb@        ret$y <- cut(ret$y, breaks = c(-Inf, cumsum(prob[,b])), @\\
\mbox{}\verb@                     labels = paste0("Y", 1:nrow(prob)), ordered_result = TRUE)@\\
\mbox{}\verb@        ret@\\
\mbox{}\verb@    }))@\\
\mbox{}\verb@    return(ret)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb20a}{20a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
<<rfree1way>>=
(logOR <- c(log(1.5), log(2)))
nd <- rfree1way(150, delta = logOR)
coef(ft <- free1way(y ~ trt, data = nd))
sqrt(diag(vcov(ft)))
logLik(ft)
nd$y <- qchisq(nd$y, df = 3)
coef(ft <- free1way(y ~ trt, data = nd))
sqrt(diag(vcov(ft)))
logLik(ft)
N <- 25
pvals <- replicate(Nsim, 
{
  nd <- rfree1way(n = N, blocks = 2, delta = c(.25, .5), alloc_ratio = 2)
  summary(free1way(y ~ trt | blk, data = nd), test = "Permutation")$p.value
})

power.free1way.test(n = N, blocks = 2, delta = c(.25, .5), alloc_ratio = 2)
mean(pvals < .05)
@

This function can also be used to simulate survival times, for example, from
a proportional hazards model with a censoring probability of $.25$ in the
control arm and of $.5$ in the treated arm, under random censoring (that is,
event and censoring times are independent given treatment).

<<rfree1waysurv>>=
N <- 1000
nd <- rfree1way(N, delta = 1, link = "cloglog")
nd$C <- rfree1way(n = N, delta = 1, offset = -c(qlogis(.25), qlogis(.5)), 
                  link = "cloglog")$y
nd$y <- Surv(pmin(nd$y, nd$C), nd$y < nd$C)
### check censoring probability
1 - tapply(nd$y[,2], nd$trt, mean)
summary(free1way(y ~ trt, data = nd, link = "cloglog"))
summary(coxph(y ~ trt, data = nd))
@

Next we start implementing a function for simulating $C \times K$ tables. We need
to specify the number of observations in each treatment group (\code{c}),
the discrete distribution of the control (\code{r}), a model (\code{link}), and a
treatment effect (\code{delta}, in line with \code{power.XYZ.test}). In
essence, we draw samples from the multinomial distribution after computing
the relevant discrete density.

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap53}\raggedright\small
\NWtarget{nuweb66}{} $\langle\,${\itshape r2dsim}\nobreak\ {\footnotesize {66}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@.r2dsim <- function(n, r, c, delta = 0,@\\
\mbox{}\verb@                   link = c("logit", "probit", "cloglog", "loglog")) @\\
\mbox{}\verb@{@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (length(n <- as.integer(n)) == 0L || (n < 0) || is.na(n)) @\\
\mbox{}\verb@        stop("invalid argument 'n'")@\\
\mbox{}\verb@    colsums <- c@\\
\mbox{}\verb@    if (length(colsums[] <- as.integer(c)) <= 1L || @\\
\mbox{}\verb@        any(colsums < 0) || anyNA(colsums)) @\\
\mbox{}\verb@        stop("invalid argument 'c'")@\\
\mbox{}\verb@@\\
\mbox{}\verb@    prob <- r@\\
\mbox{}\verb@    if (length(prob[] <- as.double(r / sum(r))) <= 1L || @\\
\mbox{}\verb@        any(prob < 0) || anyNA(prob)) @\\
\mbox{}\verb@        stop("invalid argument 'r'")@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (is.null(names(prob))) @\\
\mbox{}\verb@        names(prob) <- paste0("i", seq_along(prob))@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    K <- length(colsums)@\\
\mbox{}\verb@    if (is.null(names(colsums))) @\\
\mbox{}\verb@        names(colsums) <- LETTERS[seq_len(K)]@\\
\mbox{}\verb@    delta <- rep_len(delta, K - 1L)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape link2fun}\nobreak\ {\footnotesize \NWlink{nuweb33}{33}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    p0 <- cumsum(prob)@\\
\mbox{}\verb@    h0 <- .q(link, p0[-length(p0)]) ### last element of p0 is one@\\
\mbox{}\verb@@\\
\mbox{}\verb@    h1 <- h0 - matrix(delta, nrow = length(prob) - 1L, ncol = K - 1, byrow = TRUE)@\\
\mbox{}\verb@    p1 <- rbind(.p(link, h1), 1)@\\
\mbox{}\verb@    p <- cbind(p0, p1)@\\
\mbox{}\verb@    ret <- vector(mode = "list", length = n)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    for (i in seq_len(n)) {@\\
\mbox{}\verb@        tab <- sapply(seq_len(K), function(k)@\\
\mbox{}\verb@            unclass(table(cut(runif(colsums[k]), breaks = c(-Inf, p[,k])))))@\\
\mbox{}\verb@        ret[[i]] <- as.table(array(unlist(tab), dim = c(length(prob), K), @\\
\mbox{}\verb@                          dimnames = list(names(prob), @\\
\mbox{}\verb@                                          names(colsums))))@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    return(ret)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb?}{?}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\chapter{Power and Sample Size}

The term ``distribution-free'' refers to the invariance of the reference
distribution with respect to the distribution of an absolutely continuous
outcome under control. Unfortunately, this is no longer true for
non-continuous outcomes (due to ties) and under the alternative. That means
that sample size assessments always take place under certain assumptions
regarding the outcome distribution.

With the infrastructure from Chapter~\ref{ch:rng}, 
we are now ready to put together a function for power evaluation and sample
size assessment. The core idea is to draw samples from the relevant data
(under a specific model in the alternative) and to estimate the Fisher
information of the treatment effect parameters for this configuration. The
power of the global Wald test can than be approximated by a non-central
$\chi^2$ distribution. This is much faster than approximating the power
directly. Nevertheless, this is a random experiment, so we first make
computations reproducible:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap54}\raggedright\small
\NWtarget{nuweb67}{} $\langle\,${\itshape random seed}\nobreak\ {\footnotesize {67}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@if (!exists(".Random.seed", envir = .GlobalEnv, inherits = FALSE)) @\\
\mbox{}\verb@    runif(1)@\\
\mbox{}\verb@if (is.null(seed)) @\\
\mbox{}\verb@    seed <- RNGstate <- get(".Random.seed", envir = .GlobalEnv)@\\
\mbox{}\verb@else {@\\
\mbox{}\verb@    R.seed <- get(".Random.seed", envir = .GlobalEnv)@\\
\mbox{}\verb@    set.seed(seed)@\\
\mbox{}\verb@    RNGstate <- structure(seed, kind = as.list(RNGkind()))@\\
\mbox{}\verb@    on.exit(assign(".Random.seed", R.seed, envir = .GlobalEnv))@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb?}{?}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap55}\raggedright\small
\NWtarget{nuweb68a}{} $\langle\,${\itshape power setup}\nobreak\ {\footnotesize {68a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape link2fun}\nobreak\ {\footnotesize \NWlink{nuweb33}{33}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@### matrix means control distributions in different strata@\\
\mbox{}\verb@if (!is.matrix(prob))@\\
\mbox{}\verb@    prob <- matrix(prob, nrow = NROW(prob), ncol = blocks)@\\
\mbox{}\verb@prob <- prop.table(prob, margin = 2L)@\\
\mbox{}\verb@C <- nrow(prob)@\\
\mbox{}\verb@B <- ncol(prob)@\\
\mbox{}\verb@if (is.null(colnames(prob))) @\\
\mbox{}\verb@    colnames(prob) <- paste0("stratum", seq_len(B))@\\
\mbox{}\verb@p0 <- apply(prob, 2, cumsum)@\\
\mbox{}\verb@h0 <- .q(link, p0[-nrow(p0),,drop = FALSE])@\\
\mbox{}\verb@@\\
\mbox{}\verb@@\hbox{$\langle\,${\itshape design args}\nobreak\ {\footnotesize \NWlink{nuweb62}{62}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@rownames(N) <- colnames(prob)@\\
\mbox{}\verb@ctrl <- "Control"@\\
\mbox{}\verb@dn <- dimnames(prob)@\\
\mbox{}\verb@if (!is.null(names(dn)[1L]))@\\
\mbox{}\verb@    ctrl <- names(dn)[1L]@\\
\mbox{}\verb@colnames(N) <- c(ctrl, names(delta))@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb?}{?}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
For estimating the Fisher information, we draw samples from the discrete
outcome distribution and evaluate the observed Fisher information for the,
here and now known true parameters. The average of these Fisher information
matrices is then used as an estimate for the expected Fisher information.
For small sample sizes less than $100$, we draw larger samples (at least
$1000$) and adjust the obtained Fisher information accordingly to reduce
sampling error.

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap56}\raggedright\small
\NWtarget{nuweb68b}{} $\langle\,${\itshape estimate Fisher information}\nobreak\ {\footnotesize {68b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@he <- 0@\\
\mbox{}\verb@deltamu <- delta - mu@\\
\mbox{}\verb@Nboost <- ifelse(n < 100, ceiling(1000 / n), 1)@\\
\mbox{}\verb@for (i in seq_len(nsim)) {@\\
\mbox{}\verb@    parm <- deltamu@\\
\mbox{}\verb@    x <- as.table(array(0, dim = c(C, K, B)))@\\
\mbox{}\verb@    for (b in seq_len(B)) {@\\
\mbox{}\verb@        x[,,b] <- .r2dsim(1L, r = prob[, b], c = Nboost * N[b,], @\\
\mbox{}\verb@                          delta = delta, link = link)[[1L]]@\\
\mbox{}\verb@        rs <- rowSums(x[,,b]) > 0@\\
\mbox{}\verb@        h <- h0[rs[-length(rs)], b]@\\
\mbox{}\verb@        theta <- c(h[1], log(diff(h)))@\\
\mbox{}\verb@        parm <- c(parm, theta)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    ### evaluate observed hessian for true parameters parm and x data@\\
\mbox{}\verb@    he <- he + .free1wayML(x, link = link, mu = mu, start = parm, @\\
\mbox{}\verb@                           fix = seq_along(parm))$hessian / Nboost@\\
\mbox{}\verb@}@\\
\mbox{}\verb@### estimate expected Fisher information@\\
\mbox{}\verb@he <- he / nsim@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb?}{?}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
The power function now depends on sample size (\code{n}; the number of
control observations in the first stratum), a discrete control distribution
(\code{prob}, this can be a $C \times B$ matrix for stratum-specific control
distributions), a vector of allocation ratios (\code{alloc_ratio = 2} means
control:treatment = 1:2) and the sample size ratios between strata.

The treatment effects are contained in $K - 1$ vector \code{delta}:

\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap57}\raggedright\small
\NWtarget{nuweb68c}{} $\langle\,${\itshape power call}\nobreak\ {\footnotesize {68c}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@power.free1way.test(n = n, prob = prob, alloc_ratio = alloc_ratio, @\\
\mbox{}\verb@                    strata_ratio = strata_ratio, delta = delta, mu = mu,@\\
\mbox{}\verb@                    sig.level = sig.level, link = link, @\\
\mbox{}\verb@                    alternative = alternative, @\\
\mbox{}\verb@                    nsim = nsim, seed = seed, tol = tol)$power - power@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb?}{?}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap58}\raggedright\small
\NWtarget{nuweb69}{} $\langle\,${\itshape power htest output}\nobreak\ {\footnotesize {69}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@ss <- paste(colSums(N), paste0("(", colnames(N), ")"), collapse = " + ")@\\
\mbox{}\verb@ret <- list(n = n, @\\
\mbox{}\verb@            "Total sample size" = paste(ss, "=", sum(N)),@\\
\mbox{}\verb@            power = power, @\\
\mbox{}\verb@            sig.level = sig.level)@\\
\mbox{}\verb@if (mu != 0) ret$mu <- mu@\\
\mbox{}\verb@ret[[link$parm]] <- delta@\\
\mbox{}\verb@ret$note <- "'n' is sample size in control group"@\\
\mbox{}\verb@if (B > 1) ret$note <- paste(ret$note, "of first stratum")@\\
\mbox{}\verb@alias <- link$alias@\\
\mbox{}\verb@if (length(link$alias) == 2L) alias <- alias[1L + (K > 2L)]@\\
\mbox{}\verb@ret$method <- paste(ifelse(B > 1L, "Stratified", ""), @\\
\mbox{}\verb@                    paste0(K, "-sample"), alias, @\\
\mbox{}\verb@                    "test against", link$model, "alternatives")@\\
\mbox{}\verb@class(ret) <- "power.htest"@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb?}{?}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
\begin{flushleft} \small
\begin{minipage}{\linewidth}\label{scrap59}\raggedright\small
\NWtarget{nuweb?}{} $\langle\,${\itshape power}\nobreak\ {\footnotesize {?}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@power.free1way.test <- function(n = NULL, prob = rep.int(1 / n, n), @\\
\mbox{}\verb@                                alloc_ratio = 1, strata_ratio = 1, blocks = NCOL(prob),@\\
\mbox{}\verb@                                delta = NULL, mu = 0, sig.level = .05, power = NULL,@\\
\mbox{}\verb@                                link = c("logit", "probit", "cloglog", "loglog"),@\\
\mbox{}\verb@                                alternative = c("two.sided", "less", "greater"), @\\
\mbox{}\verb@                                nsim = 100, seed = NULL, tol = .Machine$double.eps^0.25) @\\
\mbox{}\verb@{@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (sum(vapply(list(n, delta, power, sig.level), is.null, @\\
\mbox{}\verb@        NA)) != 1) @\\
\mbox{}\verb@        stop("exactly one of 'n', 'delta', 'power', and 'sig.level' must be NULL")@\\
\mbox{}\verb@    stats:::assert_NULL_or_prob(sig.level)@\\
\mbox{}\verb@    stats:::assert_NULL_or_prob(power)@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape random seed}\nobreak\ {\footnotesize \NWlink{nuweb67}{67}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape r2dsim}\nobreak\ {\footnotesize \NWlink{nuweb66}{66}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (is.null(n)) @\\
\mbox{}\verb@        n <- ceiling(uniroot(function(n) {@\\
\mbox{}\verb@                 @\hbox{$\langle\,${\itshape power call}\nobreak\ {\footnotesize \NWlink{nuweb68c}{68c}}$\,\rangle$}\verb@@\\
\mbox{}\verb@             }, interval = c(5, 1e+03), tol = tol, extendInt = "upX")$root)@\\
\mbox{}\verb@    else if (is.null(delta)) {@\\
\mbox{}\verb@        ### 2-sample only@\\
\mbox{}\verb@        stopifnot(K == 2L)@\\
\mbox{}\verb@        delta <- uniroot(function(delta) {@\\
\mbox{}\verb@                 @\hbox{$\langle\,${\itshape power call}\nobreak\ {\footnotesize \NWlink{nuweb68c}{68c}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    ### <TH> interval depending on alternative, symmetry? </TH>@\\
\mbox{}\verb@            }, interval = c(0, 10), tol = tol, extendInt = "upX")$root@\\
\mbox{}\verb@        }@\\
\mbox{}\verb@    else if (is.null(sig.level)) @\\
\mbox{}\verb@        sig.level <- uniroot(function(sig.level) {@\\
\mbox{}\verb@                @\hbox{$\langle\,${\itshape power call}\nobreak\ {\footnotesize \NWlink{nuweb68c}{68c}}$\,\rangle$}\verb@@\\
\mbox{}\verb@            }, interval = c(1e-10, 1 - 1e-10), tol = tol, extendInt = "yes")$root@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape power setup}\nobreak\ {\footnotesize \NWlink{nuweb68a}{68a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape estimate Fisher information}\nobreak\ {\footnotesize \NWlink{nuweb68b}{68b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    alternative <- match.arg(alternative)@\\
\mbox{}\verb@    if (K == 2L) {@\\
\mbox{}\verb@        se <- 1 / sqrt(c(he))@\\
\mbox{}\verb@        power  <- switch(alternative, @\\
\mbox{}\verb@            "two.sided" = pnorm(qnorm(sig.level / 2) + deltamu / se) + @\\
\mbox{}\verb@                          pnorm(qnorm(sig.level / 2) - deltamu / se),@\\
\mbox{}\verb@            "less" = pnorm(qnorm(sig.level) - deltamu / se),@\\
\mbox{}\verb@            "greater" = pnorm(qnorm(sig.level) + deltamu / se)@\\
\mbox{}\verb@        )@\\
\mbox{}\verb@    } else {@\\
\mbox{}\verb@        stopifnot(alternative == "two.sided")@\\
\mbox{}\verb@        ncp <- sum((chol(he) %*% deltamu)^2)@\\
\mbox{}\verb@        qsig <- qchisq(sig.level, df = K - 1L, lower.tail = FALSE)@\\
\mbox{}\verb@        power <- pchisq(qsig, df = K - 1L, ncp = ncp, lower.tail = FALSE)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape power htest output}\nobreak\ {\footnotesize \NWlink{nuweb69}{69}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    ret@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb20a}{20a}.

\item{}
\end{list}
\end{minipage}\vspace{4ex}
\end{flushleft}
We start with the power of a binomial experiment with $N = 2 \times 25$
observations. In the control group, the odds of winning is 1. Under
treatment, we increase this odds by $50\%$. We compare the results with
\code{power.prop.test}:

<<power.prop.test>>=
delta <- log(1.5)
power.prop.test(n = 25, p1 = .5, p2 = plogis(qlogis(.5) - delta))
power.free1way.test(n = 25, prob = c(.5, .5), delta = delta)
@

Under stratification (twice as many observations in the second stratum) 
and with an ordered outcome at four levels, we might want to compare four
groups, with $25\%$, $50\%$, and $75\%$ increase compared to the odds of the
control:
<<power.odds.test>>=
prb <- matrix(c(.25, .25, .25, .25,
                .10, .20, .30, .40), ncol = 2)
colnames(prb) <- c("s1", "s2")
power.free1way.test(n = 20, prob = prb, 
                    strata_ratio = 2,
                    alloc_ratio = c(1.5, 2, 2), 
                    delta = log(c("low" = 1.25, "med" = 1.5, "high" = 1.75)))
@

We now estimate the power of a Wilcoxon test with, first by simulation from
a logistic distribution, and then by our power function:

<<wilcox>>=

delta <- log(3)
N <- 15
w <- gl(2, N)
pw <- numeric(Nsim)
for (i in seq_along(pw)) {
    y <- rlogis(length(w), location = c(0, delta)[w])
    pw[i] <- wilcox.test(y ~ w)$p.value
}
mean(pw < .05)

power.free1way.test(n = N, delta = delta)
@

The power of the Kruskal-Wallis test only needs one additional treatment
effect

<<kruskal>>=
delta <- c("B" = log(2), "C" = log(3))
N <- 15
w <- gl(3, N)
pw <- numeric(Nsim)
for (i in seq_along(pw)) {
    y <- rlogis(length(w), location = c(0, delta)[w])
    pw[i] <- kruskal.test(y ~ w)$p.value
}
mean(pw < .05)

power.free1way.test(n = N, delta = delta)
@

We next use the \code{rfree1way} function to sample from $4 \times 3$ tables with odds ratios $2$ and $3$
and compare the resulting power with result obtained from the approximated
Fisher information. By default, the continuous control distribution is
uniform on the unit interval, thus \code{cut} with breaks defined by the
target control discrete probability distribution generates the outcome.
The plot shows the distribution of the parameter
estimates and the corresponding population values as red dots.	

<<table, fig = TRUE>>=
prb <- rep.int(1, 4) / 4
pw <- numeric(Nsim)
cf <- matrix(0, nrow = Nsim, ncol = length(delta))
colnames(cf) <- names(delta)
for (i in seq_along(pw)) {
    nd <- rfree1way(n = N, prob = prb, delta = delta)
    ft <- free1way(y ~ trt, data = nd)
    cf[i,] <- coef(ft)
    pw[i] <- summary(ft, test = "Permutation")$p.value
}
mean(pw < .05)
boxplot(cf)
points(c(1:2), delta, pch = 19, col = "red")
power.free1way.test(n = N, prob = prb, delta = delta)
@

In the last example, we sample from $4 \times 3$ tables with odds ratios $2$ and $3$ for three
strata with different control distributions, and again compare the
simulation results to the power function:

<<stable, fig = TRUE>>=
prb <- cbind(S1 = rep(1, 4), 
             S2 = c(1, 2, 1, 2), 
             S3 = 1:4)
dimnames(prb) <- list(Ctrl = paste0("i", seq_len(nrow(prb))),
                      Strata = colnames(prb))

pw <- numeric(Nsim)
cf <- matrix(0, nrow = Nsim, ncol = length(delta))
colnames(cf) <- names(delta)
for (i in seq_along(pw)) {
    nd <- rfree1way(n = N, prob = prb, delta = delta)
    ft <- free1way(y ~ trt | blk, data = nd)
    cf[i,] <- coef(ft)
    pw[i] <- summary(ft, test = "Permutation")$p.value
}
mean(pw < .05)
boxplot(cf)
points(c(1:2), delta, pch = 19, col = "red")

power.free1way.test(n = N, prob = prb, delta = delta, seed = 3)
power.free1way.test(power = .8, prob = prb, delta = delta, seed = 3)
power.free1way.test(n = 19, prob = prb, delta = delta, seed = 3)
@

\chapter*{Index}

\section*{Files}


{\small\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \verb@"free1way.R"@ {\footnotesize {\NWtxtDefBy} \NWlink{nuweb20a}{20a}.}
\item \verb@"linkfun.R"@ {\footnotesize {\NWtxtDefBy} \NWlink{nuweb14}{14}.}
\end{list}}

\section*{Fragments}


{\small\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item $\langle\,$cloglog\nobreak\ {\footnotesize \NWlink{nuweb18}{18}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb14}{14}.}
\item $\langle\,$cumsumrev\nobreak\ {\footnotesize \NWlink{nuweb4c}{4c}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$density prob ratio\nobreak\ {\footnotesize \NWlink{nuweb4a}{4a}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb4b}{4b}\NWlink{nuweb5a}{, 5a}.
}
\item $\langle\,$design args\nobreak\ {\footnotesize \NWlink{nuweb62}{62}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb65}{65}\NWlink{nuweb68a}{, 68a}.
}
\item $\langle\,$diagonal elements for Hessian of intercepts\nobreak\ {\footnotesize \NWlink{nuweb6a}{6a}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb7}{7}.}
\item $\langle\,$do optim\nobreak\ {\footnotesize \NWlink{nuweb21}{21}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb22}{22}\NWlink{nuweb23}{, 23}.
}
\item $\langle\,$estimate Fisher information\nobreak\ {\footnotesize \NWlink{nuweb68b}{68b}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb?}{?}.}
\item $\langle\,$free1way\nobreak\ {\footnotesize \NWlink{nuweb34}{34}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb20a}{20a}.}
\item $\langle\,$free1way confint\nobreak\ {\footnotesize \NWlink{nuweb42}{42}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb20a}{20a}.}
\item $\langle\,$free1way factor\nobreak\ {\footnotesize \NWlink{nuweb38}{38}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb20a}{20a}.}
\item $\langle\,$free1way formula\nobreak\ {\footnotesize \NWlink{nuweb36}{36}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb20a}{20a}.}
\item $\langle\,$free1way methods\nobreak\ {\footnotesize \NWlink{nuweb39}{39}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb20a}{20a}.}
\item $\langle\,$free1way numeric\nobreak\ {\footnotesize \NWlink{nuweb37b}{37b}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb20a}{20a}.}
\item $\langle\,$free1way print\nobreak\ {\footnotesize \NWlink{nuweb40}{40}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb20a}{20a}.}
\item $\langle\,$free1way summary\nobreak\ {\footnotesize \NWlink{nuweb41}{41}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb20a}{20a}.}
\item $\langle\,$Hessian\nobreak\ {\footnotesize \NWlink{nuweb7}{7}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$Hessian prep\nobreak\ {\footnotesize \NWlink{nuweb5b}{5b}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb7}{7}.}
\item $\langle\,$intercept / shift contributions to Hessian\nobreak\ {\footnotesize \NWlink{nuweb6b}{6b}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb7}{7}.}
\item $\langle\,$link2fun\nobreak\ {\footnotesize \NWlink{nuweb33}{33}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb34}{34}\NWlink{nuweb65}{, 65}\NWlink{nuweb66}{, 66}\NWlink{nuweb68a}{, 68a}.
}
\item $\langle\,$linkfun\nobreak\ {\footnotesize \NWlink{nuweb15}{15}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb14}{14}.}
\item $\langle\,$logit\nobreak\ {\footnotesize \NWlink{nuweb16}{16}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb14}{14}.}
\item $\langle\,$loglog\nobreak\ {\footnotesize \NWlink{nuweb17}{17}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb14}{14}.}
\item $\langle\,$LRT\nobreak\ {\footnotesize \NWlink{nuweb28a}{28a}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb27a}{27a}.}
\item $\langle\,$ML estimation\nobreak\ {\footnotesize \NWlink{nuweb25}{25}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb20a}{20a}.}
\item $\langle\,$negative logLik\nobreak\ {\footnotesize \NWlink{nuweb3b}{3b}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$negative score\nobreak\ {\footnotesize \NWlink{nuweb4b}{4b}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$negative score residuals\nobreak\ {\footnotesize \NWlink{nuweb5a}{5a}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$off-diagonal elements for Hessian of intercepts\nobreak\ {\footnotesize \NWlink{nuweb5c}{5c}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb7}{7}.}
\item $\langle\,$optim\nobreak\ {\footnotesize \NWlink{nuweb23}{23}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$parm to prob\nobreak\ {\footnotesize \NWlink{nuweb3a}{3a}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb3b}{3b}\NWlink{nuweb4b}{, 4b}\NWlink{nuweb5a}{, 5a}\NWlink{nuweb7}{, 7}.
}
\item $\langle\,$Permutation\nobreak\ {\footnotesize \NWlink{nuweb29}{29}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb27a}{27a}.}
\item $\langle\,$post processing\nobreak\ {\footnotesize \NWlink{nuweb24}{24}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$power\nobreak\ {\footnotesize \NWlink{nuweb?}{?}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb20a}{20a}.}
\item $\langle\,$power call\nobreak\ {\footnotesize \NWlink{nuweb68c}{68c}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb?}{?}.}
\item $\langle\,$power htest output\nobreak\ {\footnotesize \NWlink{nuweb69}{69}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb?}{?}.}
\item $\langle\,$power setup\nobreak\ {\footnotesize \NWlink{nuweb68a}{68a}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb?}{?}.}
\item $\langle\,$ppplot\nobreak\ {\footnotesize \NWlink{nuweb58}{58}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb20a}{20a}.}
\item $\langle\,$probit\nobreak\ {\footnotesize \NWlink{nuweb19}{19}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb14}{14}.}
\item $\langle\,$profile\nobreak\ {\footnotesize \NWlink{nuweb22}{22}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$r2dsim\nobreak\ {\footnotesize \NWlink{nuweb66}{66}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb?}{?}.}
\item $\langle\,$random seed\nobreak\ {\footnotesize \NWlink{nuweb67}{67}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb?}{?}.}
\item $\langle\,$Rao\nobreak\ {\footnotesize \NWlink{nuweb28b}{28b}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb27a}{27a}.}
\item $\langle\,$resampling\nobreak\ {\footnotesize \NWlink{nuweb31}{31}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb34}{34}.}
\item $\langle\,$rfree1way\nobreak\ {\footnotesize \NWlink{nuweb65}{65}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb20a}{20a}.}
\item $\langle\,$ROC bands\nobreak\ {\footnotesize \NWlink{nuweb57}{57}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb58}{58}.}
\item $\langle\,$setup and starting values\nobreak\ {\footnotesize \NWlink{nuweb20b}{20b}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$statistics\nobreak\ {\footnotesize \NWlink{nuweb27a}{27a}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb40}{40}\NWlink{nuweb42}{, 42}.
}
\item $\langle\,$Strasser Weber\nobreak\ {\footnotesize \NWlink{nuweb30}{30}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb34}{34}.}
\item $\langle\,$stratified Hessian\nobreak\ {\footnotesize \NWlink{nuweb12}{12}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$stratified negative logLik\nobreak\ {\footnotesize \NWlink{nuweb10c}{10c}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$stratified negative score\nobreak\ {\footnotesize \NWlink{nuweb11a}{11a}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$stratified negative score residual\nobreak\ {\footnotesize \NWlink{nuweb11b}{11b}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb25}{25}.}
\item $\langle\,$stratum prep\nobreak\ {\footnotesize \NWlink{nuweb10b}{10b}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb10c}{10c}\NWlink{nuweb11a}{, 11a}\NWlink{nuweb11b}{b}\NWlink{nuweb12}{, 12}.
}
\item $\langle\,$table2list\nobreak\ {\footnotesize \NWlink{nuweb10a}{10a}}$\,\rangle$ {\footnotesize {\NWtxtNoRef}.}
\item $\langle\,$table2list body\nobreak\ {\footnotesize \NWlink{nuweb9}{9}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb10a}{10a}\NWlink{nuweb20b}{, 20b}.
}
\item $\langle\,$variable names and checks\nobreak\ {\footnotesize \NWlink{nuweb37a}{37a}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb37b}{37b}\NWlink{nuweb38}{, 38}.
}
\item $\langle\,$Wald statistic\nobreak\ {\footnotesize \NWlink{nuweb27b}{27b}}$\,\rangle$ {\footnotesize {\NWtxtRefIn} \NWlink{nuweb27a}{27a}.}
\end{list}}

\section*{Identifiers}



\bibliographystyle{plainnat}
\bibliography{\Sexpr{gsub("\\.bib", "", system.file("refs.bib", package = "free1way"))}}

\end{document}
